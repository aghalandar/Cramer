{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aghalandar/Cramer/blob/main/Hangman_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trexquant Interview Project (The Hangman Game)\n",
        "\n",
        "*   Copyright Trexquant Investment LP. All Rights Reserved.\n",
        "*   Redistribution of this question without written consent from Trexquant is prohibited"
      ],
      "metadata": {
        "id": "CaUGAdRR6eVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction:\n",
        "For this coding test, your mission is to write an algorithm that plays the game of Hangman through our API server.\n",
        "\n",
        "When a user plays Hangman, the server first selects a secret word at random from a list. The server then returns a row of underscores (space separated)—one for each letter in the secret word—and asks the user to guess a letter. If the user guesses a letter that is in the word, the word is redisplayed with all instances of that letter shown in the correct positions, along with any letters correctly guessed on previous turns. If the letter does not appear in the word, the user is charged with an incorrect guess. The user keeps guessing letters until either (1) the user has correctly guessed all the letters in the word or (2) the user has made six incorrect guesses.\n",
        "\n",
        "You are required to write a \"guess\" function that takes current word (with underscores) as input and returns a guess letter. You will use the API codes below to play 1,000 Hangman games. You have the opportunity to practice before you want to start recording your game results.\n",
        "\n",
        "Your algorithm is permitted to use a training set of approximately 250,000 dictionary words. Your algorithm will be tested on an entirely disjoint set of 250,000 dictionary words. Please note that this means the words that you will ultimately be tested on do NOT appear in the dictionary that you are given. You are not permitted to use any dictionary other than the training dictionary we provided. This requirement will be strictly enforced by code review.\n",
        "\n",
        "You are provided with a basic, working algorithm. This algorithm will match the provided masked string (e.g. a _ _ l e) to all possible words in the dictionary, tabulate the frequency of letters appearing in these possible words, and then guess the letter with the highest frequency of appearence that has not already been guessed. If there are no remaining words that match then it will default back to the character frequency distribution of the entire dictionary.\n",
        "\n",
        "This benchmark strategy is successful approximately 18% of the time. Your task is to design an algorithm that significantly outperforms this benchmark."
      ],
      "metadata": {
        "id": "RHVr8RpMb2zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from itertools import combinations\n",
        "import json\n",
        "import requests\n",
        "import random\n",
        "import string\n",
        "import secrets\n",
        "import time\n",
        "import re\n",
        "import collections\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "try:\n",
        "    from urllib.parse import parse_qs, urlencode, urlparse\n",
        "except ImportError:\n",
        "    from urlparse import parse_qs, urlparse\n",
        "    from urllib import urlencode\n",
        "\n",
        "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
        "\n",
        "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
      ],
      "metadata": {
        "id": "a9LEdV0s6cWF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data():\n",
        "    # Function to read data from a file\n",
        "    with open(\"/content/words_250000_train.txt\", \"r\") as f:\n",
        "        df = f.read()\n",
        "    return df\n",
        "\n",
        "def generate_ngrams(word, n):\n",
        "    \"\"\"\n",
        "    Function to generate all possible n-grams from a given word.\n",
        "\n",
        "    Parameters:\n",
        "    - word (str): The input word.\n",
        "    - n (int): The size of the n-grams.\n",
        "\n",
        "    Returns:\n",
        "    - List of n-grams.\n",
        "    \"\"\"\n",
        "    ngrams = [word[i:i+n] for i in range(len(word) - n + 1)]\n",
        "    return ngrams\n",
        "\n",
        "def create_ngram_dictionary(df, n):\n",
        "    \"\"\"\n",
        "    Function to create a dictionary of n-gram combinations for each word in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - df (DataFrame): The input DataFrame containing words.\n",
        "    - n (int): The size of the n-grams.\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary where each word is mapped to a list of encoded n-gram combinations.\n",
        "    \"\"\"\n",
        "    ngram_dictionary = {}\n",
        "    counter = 0\n",
        "\n",
        "    for word in df[0]:\n",
        "        all_ngrams_for_word = []\n",
        "\n",
        "        # Generate n-grams for the word\n",
        "        ngrams = generate_ngrams(word, n)\n",
        "\n",
        "        # Encode each n-gram\n",
        "        encoded_ngrams = [ngram for ngram in ngrams]\n",
        "\n",
        "        # Append the encoded n-grams to the list\n",
        "        all_ngrams_for_word += encoded_ngrams\n",
        "\n",
        "        # Map the original word to the list of encoded n-gram combinations in the dictionary\n",
        "        ngram_dictionary[word] = all_ngrams_for_word\n",
        "\n",
        "    return ngram_dictionary\n",
        "\n",
        "def generate_filled_combinations_for_list(word_list):\n",
        "    \"\"\"\n",
        "    Generates filled combinations for words in a given list by replacing letters with underscores.\n",
        "\n",
        "    Parameters:\n",
        "    - word_list (list): List of words for which filled combinations are generated.\n",
        "\n",
        "    Returns:\n",
        "    - all_filled_combinations (dict): Dictionary where keys are words, and values are lists of filled combinations.\n",
        "    \"\"\"\n",
        "    # Initialize an empty dictionary to store filled combinations for each word\n",
        "    all_filled_combinations = {}\n",
        "\n",
        "    # Iterate through each word in the given list\n",
        "    for word in word_list:\n",
        "        # Iterate over the number of underscores starting from 1\n",
        "        for num_underscores in range(1, len(word)):\n",
        "            # Generate all possible pairs of positions to replace letters with underscores\n",
        "            positions = list(range(len(word)))\n",
        "            pairs = list(combinations(positions, num_underscores))\n",
        "\n",
        "            # Generate filled combinations for each pair of positions and add underscores in between\n",
        "            filled_combinations = [\n",
        "                \"\".join(word[idx] if idx in pair else '_' for idx in range(len(word))) for pair in pairs\n",
        "            ]\n",
        "\n",
        "            # Add the filled combinations to the overall list for the current word\n",
        "            all_filled_combinations.setdefault(word, []).extend(filled_combinations)\n",
        "\n",
        "    return all_filled_combinations\n",
        "\n",
        "\n",
        "\n",
        "def create_char_mapping():\n",
        "    \"\"\"\n",
        "    Creates a character-to-index mapping and an index-to-character mapping for a predefined set of characters.\n",
        "\n",
        "    Returns:\n",
        "    - char_to_index (dict): Dictionary mapping characters to their corresponding indices.\n",
        "    - index_to_char (dict): Dictionary mapping indices to their corresponding characters.\n",
        "    \"\"\"\n",
        "    # Define a set of characters including lowercase letters, an underscore, and an asterisk\n",
        "    chars = \"abcdefghijklmnopqrstuvwxyz_*\"\n",
        "\n",
        "    # Create a character-to-index mapping\n",
        "    char_to_index = {char: i for i, char in enumerate(chars)}\n",
        "\n",
        "    # Create an index-to-character mapping\n",
        "    index_to_char = {i: char for i, char in enumerate(chars)}\n",
        "\n",
        "    return char_to_index, index_to_char\n",
        "\n",
        "def encode_input(word):\n",
        "    \"\"\"\n",
        "    Encodes the input word into a numerical vector of fixed length (6).\n",
        "\n",
        "    Parameters:\n",
        "    - word (str): The input word to be encoded.\n",
        "\n",
        "    Returns:\n",
        "    - word_vector (list): Numerical vector representing the encoded input word.\n",
        "    \"\"\"\n",
        "    # Create a character-to-index mapping and an underscore placeholder\n",
        "    char_to_index, _ = create_char_mapping()\n",
        "\n",
        "    # Fixed length of the input vector\n",
        "    embedding_len = 6\n",
        "\n",
        "    # Initialize an input vector with zeros\n",
        "    word_vector = [0] * embedding_len\n",
        "\n",
        "    # Iterate through each letter in the input word and set the corresponding position in the vector\n",
        "    for letter_no in range(embedding_len):\n",
        "        if letter_no < len(word):\n",
        "            word_vector[letter_no] = char_to_index[word[letter_no]]\n",
        "        else:\n",
        "            # If the word is shorter than the fixed length, pad with an underscore placeholder\n",
        "            word_vector[letter_no] = char_to_index['*']\n",
        "\n",
        "    return word_vector\n",
        "\n",
        "\n",
        "def encode_output(word):\n",
        "    \"\"\"\n",
        "    Encodes the output word into a numerical vector using a character mapping.\n",
        "\n",
        "    Parameters:\n",
        "    - word (str): The output word to be encoded.\n",
        "\n",
        "    Returns:\n",
        "    - output_vector (list): Numerical vector representing the encoded output word.\n",
        "    \"\"\"\n",
        "    # Create a character mapping and an underscore placeholder\n",
        "    char_mapping, _ = create_char_mapping()\n",
        "\n",
        "    # Initialize an output vector with zeros for each letter of the alphabet\n",
        "    output_vector = [0] * 26\n",
        "\n",
        "    # Iterate through each letter in the word and set the corresponding position in the vector to 1\n",
        "    for letter in word:\n",
        "        output_vector[char_mapping[letter]] = 1\n",
        "\n",
        "    return output_vector\n",
        "\n",
        "def encode_dictionary(masked_dictionary):\n",
        "    \"\"\"\n",
        "    Encodes words into numerical vectors for machine learning.\n",
        "\n",
        "    Parameters:\n",
        "    - masked_dictionary (dict): A dictionary where keys are output words and values are lists of input words.\n",
        "\n",
        "    Returns:\n",
        "    - input_data (list): List containing encoded numerical vectors representing input words.\n",
        "    - target_data (list): List containing encoded numerical vectors representing corresponding output words.\n",
        "    \"\"\"\n",
        "    # Initialize empty lists to store encoded input and output vectors\n",
        "    target_data = []\n",
        "    input_data = []\n",
        "    counter = 0\n",
        "\n",
        "    # Iterate through the masked dictionary\n",
        "    for output_word, input_words in masked_dictionary.items():\n",
        "        # Encode the output word\n",
        "        output_vector = encode_output(output_word)\n",
        "\n",
        "        # Iterate through the input words and encode them\n",
        "        for input_word in input_words:\n",
        "            target_data.append(output_vector)\n",
        "            input_data.append(encode_input(input_word))\n",
        "\n",
        "    return input_data, target_data\n",
        "\n",
        "def convert_to_tensor(input_data, target_data):\n",
        "    \"\"\"\n",
        "    Converts input and target data to PyTorch tensors.\n",
        "\n",
        "    Parameters:\n",
        "    - input_data (list): List containing input data in the form of encoded sequences.\n",
        "    - target_data (list): List containing target data in the form of encoded sequences.\n",
        "\n",
        "    Returns:\n",
        "    - input_tensor (torch.Tensor): PyTorch tensor representing the input data.\n",
        "    - target_tensor (torch.Tensor): PyTorch tensor representing the target data.\n",
        "    \"\"\"\n",
        "    # Convert input_data and target_data to PyTorch tensors with data type torch.long\n",
        "    input_tensor = torch.tensor(input_data, dtype=torch.long)\n",
        "    target_tensor = torch.tensor(target_data, dtype=torch.float32)\n",
        "\n",
        "    return input_tensor, target_tensor\n",
        "\n",
        "def save_input_output_data(input_data, target_data):\n",
        "    \"\"\"\n",
        "    Saves input and target data to text files.\n",
        "\n",
        "    Parameters:\n",
        "    - input_data (list): List containing input data.\n",
        "    - target_data (list): List containing target data.\n",
        "    \"\"\"\n",
        "    # Save input data to 'input_features.txt'\n",
        "    with open(r'input_features.txt', 'w') as fp:\n",
        "        for item in input_data:\n",
        "            # Write each item on a new line\n",
        "            fp.write(\"%s\\n\" % item)\n",
        "        print('Input data saved successfully.')\n",
        "\n",
        "    # Save target data to 'target_features.txt'\n",
        "    with open(r'target_features.txt', 'w') as fp:\n",
        "        for item in target_data:\n",
        "            # Write each item on a new line\n",
        "            fp.write(\"%s\\n\" % item)\n",
        "        print('Target data saved successfully.')\n",
        "\n",
        "\n",
        "def get_datasets():\n",
        "    \"\"\"\n",
        "    Processes and prepares datasets for machine learning training.\n",
        "\n",
        "    Reads data, generates n-grams from 2 to 6, creates n-gram dictionaries, flattens and concatenates n-grams,\n",
        "    encodes n-grams into numerical vectors, and converts data to PyTorch tensors.\n",
        "\n",
        "    Returns:\n",
        "    - input_tensor (torch.Tensor): PyTorch tensor representing the input data.\n",
        "    - target_tensor (torch.Tensor): PyTorch tensor representing the target data.\n",
        "    \"\"\"\n",
        "    # Reading the data\n",
        "    df = read_data()\n",
        "    x = pd.DataFrame(df.split('\\n'))\n",
        "\n",
        "    # Initialize empty lists to store input and target data\n",
        "    input_data = []\n",
        "    target_data = []\n",
        "\n",
        "    # Loop through n-grams from 2 to 6\n",
        "    for ngram in range(2, 7):\n",
        "        # Create n-gram dictionary\n",
        "        result_ngrams = create_ngram_dictionary(x, ngram)\n",
        "\n",
        "        # Flatten and concatenate n-grams\n",
        "        result_ngrams_list = list(set(perm for perms_list in result_ngrams.values() for perm in perms_list))\n",
        "        all_permutations = generate_filled_combinations_for_list(result_ngrams_list)\n",
        "\n",
        "        # Encode n-grams\n",
        "        current_input_data, current_target_data = encode_dictionary(all_permutations)\n",
        "\n",
        "        # Append to the overall lists\n",
        "        input_data.extend(current_input_data)\n",
        "        target_data.extend(current_target_data)\n",
        "        print(f'{ngram}-gram is Done!')\n",
        "\n",
        "    save_input_output_data(input_data, target_data)\n",
        "    # Convert to tensors\n",
        "    input_tensor, target_tensor = convert_to_tensor(input_data, target_data)\n",
        "    print(input_tensor.size(), target_tensor.size())\n",
        "\n",
        "    return input_tensor, target_tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "aAfYrrk2cl0Y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the data for LSTM\n",
        "input_tensor, target_tensor = get_datasets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsgDfkW1cowA",
        "outputId": "93e56c4f-2f1f-4d80-d4da-b5c2eaadccc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2-gram is Done!\n",
            "3-gram is Done!\n",
            "4-gram is Done!\n",
            "5-gram is Done!\n",
            "6-gram is Done!\n",
            "Input data saved successfully.\n",
            "Target data saved successfully.\n",
            "torch.Size([29977280, 6]) torch.Size([29977280, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the Model\n",
        "def train_loop(data_loader, model, loss_fn, optimizer, loss_estimate, batch_no, epoch, epoch_no):\n",
        "    \"\"\"\n",
        "    Training loop for the machine learning model.\n",
        "\n",
        "    Parameters:\n",
        "    - data_loader (torch.utils.data.DataLoader): DataLoader containing training data.\n",
        "    - model (torch.nn.Module): The machine learning model to be trained.\n",
        "    - loss_fn: The loss function used for training.\n",
        "    - optimizer: The optimization algorithm for updating model parameters.\n",
        "    - loss_estimate (list): List to store loss values for visualization.\n",
        "    - batch_no (list): List to store batch numbers for visualization.\n",
        "    - epoch (int): The current epoch number.\n",
        "    - epoch_no (list): List to store epoch numbers for visualization.\n",
        "    \"\"\"\n",
        "    size = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    # Iterate through batches in the data loader\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # Forward pass\n",
        "        pred = model(X)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Logging and visualization\n",
        "        if batch % 1000 == 0:\n",
        "            loss_value, current_batch = loss.item(), (batch + 1) * len(X)\n",
        "\n",
        "            # Append values for visualization\n",
        "            loss_estimate.append(loss_value)\n",
        "            batch_no.append(current_batch)\n",
        "            epoch_no.append(epoch)\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"loss: {loss_value:>7f}  [{current_batch:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(data_loader, model, loss_fn):\n",
        "    \"\"\"\n",
        "    Testing loop for evaluating the performance of a machine learning model on a test dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - data_loader (torch.utils.data.DataLoader): DataLoader containing test data.\n",
        "    - model (torch.nn.Module): The trained machine learning model.\n",
        "    - loss_fn: The loss function used for evaluation.\n",
        "    \"\"\"\n",
        "    size = len(data_loader.dataset)\n",
        "    model.eval()\n",
        "    num_batches = len(data_loader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Disable gradient computation during evaluation\n",
        "    with torch.no_grad():\n",
        "        # Iterate through batches in the data loader\n",
        "        for (X, y) in data_loader:\n",
        "            # Forward pass\n",
        "            pred = model(X)\n",
        "\n",
        "            # Compute test loss\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "\n",
        "            # Calculate the number of correct predictions\n",
        "            correct += (pred.argmax(dim = 1) == y.argmax(dim=1)).type(torch.float).sum().item()\n",
        "\n",
        "    # Calculate average test loss and accuracy\n",
        "    test_loss /= num_batches\n",
        "    accuracy = correct / size\n",
        "\n",
        "    # Print test results\n",
        "    print(f\"Test Error: \\n Accuracy: {(100 * accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "\n",
        "class CustomDatasetTrain(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch dataset for training data.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train: Features of the training dataset.\n",
        "    - y_train: Labels of the training dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, X_train, y_train):\n",
        "        self.features = X_train\n",
        "        self.label = y_train\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a sample from the dataset given an index.\n",
        "\n",
        "        Parameters:\n",
        "        - idx (int): Index of the sample.\n",
        "\n",
        "        Returns:\n",
        "        - features (tensor): Features of the sample.\n",
        "        - label (tensor): Label of the sample.\n",
        "        \"\"\"\n",
        "        features = self.features[idx]\n",
        "        label = self.label[idx]\n",
        "        sample = {\"features\": features, \"label\": label}\n",
        "        return features, label\n",
        "\n",
        "\n",
        "class extract_tensor(nn.Module):\n",
        "    def forward(self,x):\n",
        "        # Output shape (batch, features, hidden)\n",
        "        tensor, _ = x\n",
        "        # Reshape shape (batch, hidden)\n",
        "        return tensor[:, -1, :]\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Definition of a neural network model with an LSTM stack for a specific task.\n",
        "\n",
        "    Architecture:\n",
        "    - Embedding layer with input dimension 64, output dimension 32, max_norm regularization, and L2 normalization.\n",
        "    - Bidirectional LSTM layer with input size 32, hidden size 64, 1 layer, batch-first, and 20% dropout.\n",
        "    - Custom function extract_tensor() (please provide the implementation).\n",
        "    - Linear layer with input size 128 and output size 26.\n",
        "\n",
        "    Parameters:\n",
        "    - None\n",
        "\n",
        "    Input:\n",
        "    - x (torch.Tensor): Input tensor to be processed by the neural network.\n",
        "\n",
        "    Output:\n",
        "    - logits (torch.Tensor): Output logits produced by the neural network.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.LSTM_stack = nn.Sequential(\n",
        "            nn.Embedding(36, 6, max_norm=1, norm_type=2),\n",
        "            nn.LSTM(input_size=6, hidden_size=36, num_layers=1, batch_first=True, dropout=0.2, bidirectional=True),\n",
        "            extract_tensor(),  # Please provide the implementation of extract_tensor()\n",
        "            nn.Linear(72, 26)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.LSTM_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def create_dataloader(input_tensor, target_tensor):\n",
        "    all_features_data = CustomDatasetTrain(input_tensor, target_tensor)\n",
        "    all_features_dataloader = DataLoader(all_features_data, batch_size=128, shuffle=True)\n",
        "    return all_features_dataloader\n",
        "\n",
        "def save_model(model):\n",
        "    torch.save(model.state_dict(), \"lstm_ngram_2.pt\")\n",
        "\n",
        "def train_model(input_tensor, target_tensor):\n",
        "    \"\"\"\n",
        "    Trains a neural network model using the specified input and target tensors.\n",
        "\n",
        "    Parameters:\n",
        "    - input_tensor (torch.Tensor): Input data tensor.\n",
        "    - target_tensor (torch.Tensor): Target data tensor.\n",
        "    \"\"\"\n",
        "    # Create a DataLoader for the training data\n",
        "    all_features_dataloader = create_dataloader(input_tensor, target_tensor)\n",
        "\n",
        "    # Initialize the neural network model\n",
        "    model = NeuralNetwork()\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Lists for storing loss, batch, and epoch values for visualization\n",
        "    loss_estimate = []\n",
        "    batch_no = []\n",
        "    epoch_no = []\n",
        "\n",
        "    # Number of training epochs\n",
        "    epochs = 5\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
        "\n",
        "        # Train the model\n",
        "        train_loop(all_features_dataloader, model, loss_fn, optimizer, loss_estimate, batch_no, epoch, epoch_no)\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        test_loop(all_features_dataloader, model, loss_fn)\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    # Save the trained model\n",
        "    save_model(model)\n"
      ],
      "metadata": {
        "id": "xwGwe8pldbXW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "train_model(input_tensor, target_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMxocGfmiCO2",
        "outputId": "485cb3af-4166-449d-a8c1-20c73347e92a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 16.674883  [  128/29977280]\n",
            "loss: 16.267006  [128128/29977280]\n",
            "loss: 15.784765  [256128/29977280]\n",
            "loss: 15.513313  [384128/29977280]\n",
            "loss: 15.323220  [512128/29977280]\n",
            "loss: 15.203616  [640128/29977280]\n",
            "loss: 15.313610  [768128/29977280]\n",
            "loss: 15.335670  [896128/29977280]\n",
            "loss: 15.396443  [1024128/29977280]\n",
            "loss: 15.493000  [1152128/29977280]\n",
            "loss: 15.203526  [1280128/29977280]\n",
            "loss: 15.059254  [1408128/29977280]\n",
            "loss: 14.934552  [1536128/29977280]\n",
            "loss: 14.840385  [1664128/29977280]\n",
            "loss: 15.151722  [1792128/29977280]\n",
            "loss: 14.850445  [1920128/29977280]\n",
            "loss: 15.199343  [2048128/29977280]\n",
            "loss: 15.356495  [2176128/29977280]\n",
            "loss: 15.289555  [2304128/29977280]\n",
            "loss: 15.281339  [2432128/29977280]\n",
            "loss: 15.127481  [2560128/29977280]\n",
            "loss: 14.914280  [2688128/29977280]\n",
            "loss: 15.089948  [2816128/29977280]\n",
            "loss: 15.076271  [2944128/29977280]\n",
            "loss: 14.877400  [3072128/29977280]\n",
            "loss: 15.212141  [3200128/29977280]\n",
            "loss: 14.767307  [3328128/29977280]\n",
            "loss: 15.407882  [3456128/29977280]\n",
            "loss: 15.394258  [3584128/29977280]\n",
            "loss: 15.143404  [3712128/29977280]\n",
            "loss: 15.116913  [3840128/29977280]\n",
            "loss: 14.832752  [3968128/29977280]\n",
            "loss: 14.810196  [4096128/29977280]\n",
            "loss: 15.230495  [4224128/29977280]\n",
            "loss: 15.483963  [4352128/29977280]\n",
            "loss: 14.862824  [4480128/29977280]\n",
            "loss: 15.540615  [4608128/29977280]\n",
            "loss: 15.370943  [4736128/29977280]\n",
            "loss: 14.985857  [4864128/29977280]\n",
            "loss: 15.360314  [4992128/29977280]\n",
            "loss: 15.173878  [5120128/29977280]\n",
            "loss: 15.300743  [5248128/29977280]\n",
            "loss: 14.933387  [5376128/29977280]\n",
            "loss: 15.186083  [5504128/29977280]\n",
            "loss: 14.821743  [5632128/29977280]\n",
            "loss: 15.190594  [5760128/29977280]\n",
            "loss: 15.019341  [5888128/29977280]\n",
            "loss: 15.140773  [6016128/29977280]\n",
            "loss: 15.428632  [6144128/29977280]\n",
            "loss: 14.810994  [6272128/29977280]\n",
            "loss: 15.106788  [6400128/29977280]\n",
            "loss: 15.373544  [6528128/29977280]\n",
            "loss: 15.266751  [6656128/29977280]\n",
            "loss: 15.024984  [6784128/29977280]\n",
            "loss: 15.116135  [6912128/29977280]\n",
            "loss: 14.658319  [7040128/29977280]\n",
            "loss: 14.927027  [7168128/29977280]\n",
            "loss: 15.291486  [7296128/29977280]\n",
            "loss: 15.206290  [7424128/29977280]\n",
            "loss: 15.287566  [7552128/29977280]\n",
            "loss: 14.996025  [7680128/29977280]\n",
            "loss: 15.400881  [7808128/29977280]\n",
            "loss: 15.275807  [7936128/29977280]\n",
            "loss: 15.029297  [8064128/29977280]\n",
            "loss: 15.119665  [8192128/29977280]\n",
            "loss: 15.148947  [8320128/29977280]\n",
            "loss: 15.166110  [8448128/29977280]\n",
            "loss: 15.106606  [8576128/29977280]\n",
            "loss: 14.761158  [8704128/29977280]\n",
            "loss: 14.820324  [8832128/29977280]\n",
            "loss: 14.779140  [8960128/29977280]\n",
            "loss: 14.876839  [9088128/29977280]\n",
            "loss: 15.170005  [9216128/29977280]\n",
            "loss: 14.937731  [9344128/29977280]\n",
            "loss: 14.904267  [9472128/29977280]\n",
            "loss: 15.211018  [9600128/29977280]\n",
            "loss: 15.261481  [9728128/29977280]\n",
            "loss: 15.269526  [9856128/29977280]\n",
            "loss: 14.694445  [9984128/29977280]\n",
            "loss: 15.070547  [10112128/29977280]\n",
            "loss: 15.023336  [10240128/29977280]\n",
            "loss: 14.776737  [10368128/29977280]\n",
            "loss: 14.794841  [10496128/29977280]\n",
            "loss: 15.370955  [10624128/29977280]\n",
            "loss: 15.036487  [10752128/29977280]\n",
            "loss: 14.943150  [10880128/29977280]\n",
            "loss: 15.040998  [11008128/29977280]\n",
            "loss: 14.795789  [11136128/29977280]\n",
            "loss: 15.071118  [11264128/29977280]\n",
            "loss: 15.029507  [11392128/29977280]\n",
            "loss: 14.907337  [11520128/29977280]\n",
            "loss: 14.942109  [11648128/29977280]\n",
            "loss: 14.679776  [11776128/29977280]\n",
            "loss: 14.715355  [11904128/29977280]\n",
            "loss: 14.083012  [12032128/29977280]\n",
            "loss: 14.545714  [12160128/29977280]\n",
            "loss: 14.342654  [12288128/29977280]\n",
            "loss: 14.863528  [12416128/29977280]\n",
            "loss: 14.814470  [12544128/29977280]\n",
            "loss: 14.491107  [12672128/29977280]\n",
            "loss: 14.469671  [12800128/29977280]\n",
            "loss: 14.814920  [12928128/29977280]\n",
            "loss: 14.796879  [13056128/29977280]\n",
            "loss: 14.555149  [13184128/29977280]\n",
            "loss: 14.977951  [13312128/29977280]\n",
            "loss: 14.413429  [13440128/29977280]\n",
            "loss: 14.289139  [13568128/29977280]\n",
            "loss: 14.678733  [13696128/29977280]\n",
            "loss: 14.151499  [13824128/29977280]\n",
            "loss: 14.410163  [13952128/29977280]\n",
            "loss: 14.293640  [14080128/29977280]\n",
            "loss: 14.638496  [14208128/29977280]\n",
            "loss: 14.676355  [14336128/29977280]\n",
            "loss: 14.235001  [14464128/29977280]\n",
            "loss: 14.708281  [14592128/29977280]\n",
            "loss: 14.445272  [14720128/29977280]\n",
            "loss: 14.551402  [14848128/29977280]\n",
            "loss: 14.855494  [14976128/29977280]\n",
            "loss: 14.804646  [15104128/29977280]\n",
            "loss: 14.678738  [15232128/29977280]\n",
            "loss: 14.288714  [15360128/29977280]\n",
            "loss: 14.817199  [15488128/29977280]\n",
            "loss: 14.334774  [15616128/29977280]\n",
            "loss: 14.550098  [15744128/29977280]\n",
            "loss: 14.211121  [15872128/29977280]\n",
            "loss: 14.353006  [16000128/29977280]\n",
            "loss: 14.327262  [16128128/29977280]\n",
            "loss: 14.584632  [16256128/29977280]\n",
            "loss: 14.611332  [16384128/29977280]\n",
            "loss: 14.348522  [16512128/29977280]\n",
            "loss: 14.215246  [16640128/29977280]\n",
            "loss: 14.337503  [16768128/29977280]\n",
            "loss: 14.159035  [16896128/29977280]\n",
            "loss: 14.740572  [17024128/29977280]\n",
            "loss: 14.267899  [17152128/29977280]\n",
            "loss: 14.630930  [17280128/29977280]\n",
            "loss: 14.702579  [17408128/29977280]\n",
            "loss: 14.436928  [17536128/29977280]\n",
            "loss: 14.554342  [17664128/29977280]\n",
            "loss: 14.325838  [17792128/29977280]\n",
            "loss: 14.671621  [17920128/29977280]\n",
            "loss: 14.187974  [18048128/29977280]\n",
            "loss: 14.240409  [18176128/29977280]\n",
            "loss: 13.936801  [18304128/29977280]\n",
            "loss: 13.612355  [18432128/29977280]\n",
            "loss: 14.459929  [18560128/29977280]\n",
            "loss: 13.968105  [18688128/29977280]\n",
            "loss: 13.841027  [18816128/29977280]\n",
            "loss: 14.433440  [18944128/29977280]\n",
            "loss: 14.580121  [19072128/29977280]\n",
            "loss: 14.214520  [19200128/29977280]\n",
            "loss: 14.419985  [19328128/29977280]\n",
            "loss: 14.623037  [19456128/29977280]\n",
            "loss: 14.503869  [19584128/29977280]\n",
            "loss: 13.728707  [19712128/29977280]\n",
            "loss: 13.913340  [19840128/29977280]\n",
            "loss: 14.331408  [19968128/29977280]\n",
            "loss: 14.063225  [20096128/29977280]\n",
            "loss: 14.470604  [20224128/29977280]\n",
            "loss: 14.088853  [20352128/29977280]\n",
            "loss: 14.421948  [20480128/29977280]\n",
            "loss: 14.070520  [20608128/29977280]\n",
            "loss: 14.146193  [20736128/29977280]\n",
            "loss: 14.317261  [20864128/29977280]\n",
            "loss: 14.422800  [20992128/29977280]\n",
            "loss: 14.134791  [21120128/29977280]\n",
            "loss: 14.123504  [21248128/29977280]\n",
            "loss: 14.528111  [21376128/29977280]\n",
            "loss: 13.587755  [21504128/29977280]\n",
            "loss: 14.147816  [21632128/29977280]\n",
            "loss: 14.599452  [21760128/29977280]\n",
            "loss: 14.100953  [21888128/29977280]\n",
            "loss: 14.161576  [22016128/29977280]\n",
            "loss: 14.099071  [22144128/29977280]\n",
            "loss: 14.346500  [22272128/29977280]\n",
            "loss: 14.295625  [22400128/29977280]\n",
            "loss: 14.409623  [22528128/29977280]\n",
            "loss: 14.318829  [22656128/29977280]\n",
            "loss: 14.173212  [22784128/29977280]\n",
            "loss: 14.225740  [22912128/29977280]\n",
            "loss: 14.207005  [23040128/29977280]\n",
            "loss: 14.337912  [23168128/29977280]\n",
            "loss: 14.083628  [23296128/29977280]\n",
            "loss: 14.062943  [23424128/29977280]\n",
            "loss: 14.477405  [23552128/29977280]\n",
            "loss: 14.168795  [23680128/29977280]\n",
            "loss: 14.305738  [23808128/29977280]\n",
            "loss: 14.151396  [23936128/29977280]\n",
            "loss: 14.074354  [24064128/29977280]\n",
            "loss: 13.774090  [24192128/29977280]\n",
            "loss: 14.161644  [24320128/29977280]\n",
            "loss: 14.051014  [24448128/29977280]\n",
            "loss: 14.379669  [24576128/29977280]\n",
            "loss: 13.644682  [24704128/29977280]\n",
            "loss: 13.963859  [24832128/29977280]\n",
            "loss: 14.161453  [24960128/29977280]\n",
            "loss: 14.312671  [25088128/29977280]\n",
            "loss: 14.337201  [25216128/29977280]\n",
            "loss: 14.459291  [25344128/29977280]\n",
            "loss: 14.196238  [25472128/29977280]\n",
            "loss: 14.301867  [25600128/29977280]\n",
            "loss: 13.803243  [25728128/29977280]\n",
            "loss: 13.928074  [25856128/29977280]\n",
            "loss: 14.390998  [25984128/29977280]\n",
            "loss: 14.013954  [26112128/29977280]\n",
            "loss: 13.823211  [26240128/29977280]\n",
            "loss: 14.045643  [26368128/29977280]\n",
            "loss: 13.983355  [26496128/29977280]\n",
            "loss: 14.121160  [26624128/29977280]\n",
            "loss: 13.990067  [26752128/29977280]\n",
            "loss: 13.721044  [26880128/29977280]\n",
            "loss: 14.086555  [27008128/29977280]\n",
            "loss: 13.976990  [27136128/29977280]\n",
            "loss: 14.156358  [27264128/29977280]\n",
            "loss: 13.998252  [27392128/29977280]\n",
            "loss: 14.201671  [27520128/29977280]\n",
            "loss: 14.434763  [27648128/29977280]\n",
            "loss: 14.452377  [27776128/29977280]\n",
            "loss: 14.031934  [27904128/29977280]\n",
            "loss: 14.384651  [28032128/29977280]\n",
            "loss: 14.007417  [28160128/29977280]\n",
            "loss: 14.128235  [28288128/29977280]\n",
            "loss: 14.171737  [28416128/29977280]\n",
            "loss: 13.683745  [28544128/29977280]\n",
            "loss: 13.880520  [28672128/29977280]\n",
            "loss: 13.903792  [28800128/29977280]\n",
            "loss: 14.301104  [28928128/29977280]\n",
            "loss: 14.115676  [29056128/29977280]\n",
            "loss: 13.747379  [29184128/29977280]\n",
            "loss: 13.637188  [29312128/29977280]\n",
            "loss: 14.245070  [29440128/29977280]\n",
            "loss: 13.977839  [29568128/29977280]\n",
            "loss: 14.176030  [29696128/29977280]\n",
            "loss: 13.947451  [29824128/29977280]\n",
            "loss: 14.091169  [29952128/29977280]\n",
            "Test Error: \n",
            " Accuracy: 23.9%, Avg loss: 13.949177 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 13.868758  [  128/29977280]\n",
            "loss: 14.439633  [128128/29977280]\n",
            "loss: 13.573545  [256128/29977280]\n",
            "loss: 14.037987  [384128/29977280]\n",
            "loss: 14.047979  [512128/29977280]\n",
            "loss: 14.162625  [640128/29977280]\n",
            "loss: 14.146883  [768128/29977280]\n",
            "loss: 13.951013  [896128/29977280]\n",
            "loss: 13.829288  [1024128/29977280]\n",
            "loss: 13.791097  [1152128/29977280]\n",
            "loss: 13.792557  [1280128/29977280]\n",
            "loss: 13.906807  [1408128/29977280]\n",
            "loss: 14.285836  [1536128/29977280]\n",
            "loss: 13.820226  [1664128/29977280]\n",
            "loss: 13.815738  [1792128/29977280]\n",
            "loss: 13.506221  [1920128/29977280]\n",
            "loss: 14.269197  [2048128/29977280]\n",
            "loss: 13.847902  [2176128/29977280]\n",
            "loss: 13.691553  [2304128/29977280]\n",
            "loss: 14.302018  [2432128/29977280]\n",
            "loss: 13.917237  [2560128/29977280]\n",
            "loss: 13.517947  [2688128/29977280]\n",
            "loss: 13.933933  [2816128/29977280]\n",
            "loss: 13.942432  [2944128/29977280]\n",
            "loss: 13.937760  [3072128/29977280]\n",
            "loss: 13.936317  [3200128/29977280]\n",
            "loss: 14.130741  [3328128/29977280]\n",
            "loss: 13.718340  [3456128/29977280]\n",
            "loss: 13.680336  [3584128/29977280]\n",
            "loss: 14.338186  [3712128/29977280]\n",
            "loss: 13.651175  [3840128/29977280]\n",
            "loss: 13.962936  [3968128/29977280]\n",
            "loss: 13.964079  [4096128/29977280]\n",
            "loss: 13.652969  [4224128/29977280]\n",
            "loss: 13.819395  [4352128/29977280]\n",
            "loss: 14.070048  [4480128/29977280]\n",
            "loss: 13.919525  [4608128/29977280]\n",
            "loss: 13.788960  [4736128/29977280]\n",
            "loss: 13.924541  [4864128/29977280]\n",
            "loss: 13.882224  [4992128/29977280]\n",
            "loss: 13.685839  [5120128/29977280]\n",
            "loss: 14.171906  [5248128/29977280]\n",
            "loss: 13.781836  [5376128/29977280]\n",
            "loss: 14.343843  [5504128/29977280]\n",
            "loss: 13.501334  [5632128/29977280]\n",
            "loss: 13.864041  [5760128/29977280]\n",
            "loss: 13.625967  [5888128/29977280]\n",
            "loss: 13.659693  [6016128/29977280]\n",
            "loss: 13.996197  [6144128/29977280]\n",
            "loss: 14.098728  [6272128/29977280]\n",
            "loss: 13.913388  [6400128/29977280]\n",
            "loss: 13.898041  [6528128/29977280]\n",
            "loss: 13.867332  [6656128/29977280]\n",
            "loss: 14.191980  [6784128/29977280]\n",
            "loss: 14.069905  [6912128/29977280]\n",
            "loss: 14.361615  [7040128/29977280]\n",
            "loss: 13.210157  [7168128/29977280]\n",
            "loss: 14.134148  [7296128/29977280]\n",
            "loss: 13.649808  [7424128/29977280]\n",
            "loss: 13.908875  [7552128/29977280]\n",
            "loss: 13.743490  [7680128/29977280]\n",
            "loss: 14.219524  [7808128/29977280]\n",
            "loss: 13.707435  [7936128/29977280]\n",
            "loss: 13.557372  [8064128/29977280]\n",
            "loss: 13.858773  [8192128/29977280]\n",
            "loss: 13.654901  [8320128/29977280]\n",
            "loss: 13.818457  [8448128/29977280]\n",
            "loss: 13.782279  [8576128/29977280]\n",
            "loss: 14.163751  [8704128/29977280]\n",
            "loss: 13.903486  [8832128/29977280]\n",
            "loss: 13.868633  [8960128/29977280]\n",
            "loss: 13.857442  [9088128/29977280]\n",
            "loss: 14.300552  [9216128/29977280]\n",
            "loss: 13.614376  [9344128/29977280]\n",
            "loss: 13.684254  [9472128/29977280]\n",
            "loss: 13.680048  [9600128/29977280]\n",
            "loss: 14.098353  [9728128/29977280]\n",
            "loss: 13.584490  [9856128/29977280]\n",
            "loss: 14.131450  [9984128/29977280]\n",
            "loss: 13.890537  [10112128/29977280]\n",
            "loss: 13.633001  [10240128/29977280]\n",
            "loss: 13.547223  [10368128/29977280]\n",
            "loss: 13.994205  [10496128/29977280]\n",
            "loss: 13.929417  [10624128/29977280]\n",
            "loss: 13.843008  [10752128/29977280]\n",
            "loss: 13.936492  [10880128/29977280]\n",
            "loss: 13.535421  [11008128/29977280]\n",
            "loss: 14.089389  [11136128/29977280]\n",
            "loss: 13.740276  [11264128/29977280]\n",
            "loss: 13.611688  [11392128/29977280]\n",
            "loss: 13.799922  [11520128/29977280]\n",
            "loss: 13.448020  [11648128/29977280]\n",
            "loss: 13.429485  [11776128/29977280]\n",
            "loss: 14.231143  [11904128/29977280]\n",
            "loss: 13.872329  [12032128/29977280]\n",
            "loss: 14.140635  [12160128/29977280]\n",
            "loss: 14.024544  [12288128/29977280]\n",
            "loss: 14.159657  [12416128/29977280]\n",
            "loss: 13.669231  [12544128/29977280]\n",
            "loss: 13.799702  [12672128/29977280]\n",
            "loss: 13.685855  [12800128/29977280]\n",
            "loss: 13.719783  [12928128/29977280]\n",
            "loss: 14.619032  [13056128/29977280]\n",
            "loss: 13.864583  [13184128/29977280]\n",
            "loss: 13.720471  [13312128/29977280]\n",
            "loss: 13.635384  [13440128/29977280]\n",
            "loss: 13.657660  [13568128/29977280]\n",
            "loss: 13.876024  [13696128/29977280]\n",
            "loss: 13.967866  [13824128/29977280]\n",
            "loss: 13.816713  [13952128/29977280]\n",
            "loss: 13.695250  [14080128/29977280]\n",
            "loss: 13.683667  [14208128/29977280]\n",
            "loss: 13.603200  [14336128/29977280]\n",
            "loss: 13.580321  [14464128/29977280]\n",
            "loss: 13.546955  [14592128/29977280]\n",
            "loss: 13.993209  [14720128/29977280]\n",
            "loss: 13.821326  [14848128/29977280]\n",
            "loss: 13.906654  [14976128/29977280]\n",
            "loss: 13.793471  [15104128/29977280]\n",
            "loss: 14.323414  [15232128/29977280]\n",
            "loss: 14.080688  [15360128/29977280]\n",
            "loss: 13.761799  [15488128/29977280]\n",
            "loss: 13.756174  [15616128/29977280]\n",
            "loss: 13.530482  [15744128/29977280]\n",
            "loss: 13.880230  [15872128/29977280]\n",
            "loss: 13.615442  [16000128/29977280]\n",
            "loss: 13.712234  [16128128/29977280]\n",
            "loss: 13.617127  [16256128/29977280]\n",
            "loss: 13.445570  [16384128/29977280]\n",
            "loss: 13.131990  [16512128/29977280]\n",
            "loss: 13.069184  [16640128/29977280]\n",
            "loss: 13.736885  [16768128/29977280]\n",
            "loss: 13.542332  [16896128/29977280]\n",
            "loss: 13.879663  [17024128/29977280]\n",
            "loss: 13.377920  [17152128/29977280]\n",
            "loss: 13.466746  [17280128/29977280]\n",
            "loss: 13.942163  [17408128/29977280]\n",
            "loss: 14.143511  [17536128/29977280]\n",
            "loss: 13.823803  [17664128/29977280]\n",
            "loss: 13.178232  [17792128/29977280]\n",
            "loss: 13.988428  [17920128/29977280]\n",
            "loss: 13.990396  [18048128/29977280]\n",
            "loss: 13.665537  [18176128/29977280]\n",
            "loss: 14.013336  [18304128/29977280]\n",
            "loss: 13.555197  [18432128/29977280]\n",
            "loss: 13.875253  [18560128/29977280]\n",
            "loss: 13.508770  [18688128/29977280]\n",
            "loss: 13.301108  [18816128/29977280]\n",
            "loss: 13.719646  [18944128/29977280]\n",
            "loss: 13.644208  [19072128/29977280]\n",
            "loss: 13.506106  [19200128/29977280]\n",
            "loss: 13.282030  [19328128/29977280]\n",
            "loss: 13.733576  [19456128/29977280]\n",
            "loss: 13.889574  [19584128/29977280]\n",
            "loss: 13.679247  [19712128/29977280]\n",
            "loss: 13.743635  [19840128/29977280]\n",
            "loss: 13.482930  [19968128/29977280]\n",
            "loss: 13.719213  [20096128/29977280]\n",
            "loss: 13.356820  [20224128/29977280]\n",
            "loss: 13.931072  [20352128/29977280]\n",
            "loss: 13.515389  [20480128/29977280]\n",
            "loss: 13.752250  [20608128/29977280]\n",
            "loss: 13.083742  [20736128/29977280]\n",
            "loss: 13.701752  [20864128/29977280]\n",
            "loss: 13.841267  [20992128/29977280]\n",
            "loss: 13.547593  [21120128/29977280]\n",
            "loss: 13.221900  [21248128/29977280]\n",
            "loss: 13.859934  [21376128/29977280]\n",
            "loss: 13.537352  [21504128/29977280]\n",
            "loss: 13.456565  [21632128/29977280]\n",
            "loss: 13.127452  [21760128/29977280]\n",
            "loss: 13.885335  [21888128/29977280]\n",
            "loss: 13.418361  [22016128/29977280]\n",
            "loss: 13.668695  [22144128/29977280]\n",
            "loss: 13.448214  [22272128/29977280]\n",
            "loss: 13.837575  [22400128/29977280]\n",
            "loss: 13.461389  [22528128/29977280]\n",
            "loss: 12.909421  [22656128/29977280]\n",
            "loss: 13.794348  [22784128/29977280]\n",
            "loss: 13.927940  [22912128/29977280]\n",
            "loss: 13.454181  [23040128/29977280]\n",
            "loss: 13.558466  [23168128/29977280]\n",
            "loss: 13.647268  [23296128/29977280]\n",
            "loss: 13.851973  [23424128/29977280]\n",
            "loss: 13.354832  [23552128/29977280]\n",
            "loss: 13.341980  [23680128/29977280]\n",
            "loss: 13.273190  [23808128/29977280]\n",
            "loss: 13.926267  [23936128/29977280]\n",
            "loss: 13.204288  [24064128/29977280]\n",
            "loss: 13.750728  [24192128/29977280]\n",
            "loss: 13.630455  [24320128/29977280]\n",
            "loss: 13.528049  [24448128/29977280]\n",
            "loss: 13.240005  [24576128/29977280]\n",
            "loss: 13.102400  [24704128/29977280]\n",
            "loss: 13.677069  [24832128/29977280]\n",
            "loss: 13.396860  [24960128/29977280]\n",
            "loss: 13.725140  [25088128/29977280]\n",
            "loss: 13.162027  [25216128/29977280]\n",
            "loss: 13.116969  [25344128/29977280]\n",
            "loss: 13.428810  [25472128/29977280]\n",
            "loss: 13.801865  [25600128/29977280]\n",
            "loss: 13.438239  [25728128/29977280]\n",
            "loss: 13.737587  [25856128/29977280]\n",
            "loss: 13.461078  [25984128/29977280]\n",
            "loss: 13.051252  [26112128/29977280]\n",
            "loss: 13.441442  [26240128/29977280]\n",
            "loss: 13.378996  [26368128/29977280]\n",
            "loss: 13.698579  [26496128/29977280]\n",
            "loss: 13.195783  [26624128/29977280]\n",
            "loss: 13.247885  [26752128/29977280]\n",
            "loss: 13.589355  [26880128/29977280]\n",
            "loss: 13.378835  [27008128/29977280]\n",
            "loss: 13.780500  [27136128/29977280]\n",
            "loss: 13.267077  [27264128/29977280]\n",
            "loss: 13.474849  [27392128/29977280]\n",
            "loss: 13.403855  [27520128/29977280]\n",
            "loss: 13.551358  [27648128/29977280]\n",
            "loss: 14.000927  [27776128/29977280]\n",
            "loss: 13.415481  [27904128/29977280]\n",
            "loss: 13.425880  [28032128/29977280]\n",
            "loss: 13.127629  [28160128/29977280]\n",
            "loss: 13.638000  [28288128/29977280]\n",
            "loss: 13.562696  [28416128/29977280]\n",
            "loss: 13.796817  [28544128/29977280]\n",
            "loss: 13.480860  [28672128/29977280]\n",
            "loss: 13.161045  [28800128/29977280]\n",
            "loss: 12.945295  [28928128/29977280]\n",
            "loss: 13.234406  [29056128/29977280]\n",
            "loss: 13.325058  [29184128/29977280]\n",
            "loss: 13.740162  [29312128/29977280]\n",
            "loss: 13.273267  [29440128/29977280]\n",
            "loss: 13.657014  [29568128/29977280]\n",
            "loss: 13.202019  [29696128/29977280]\n",
            "loss: 13.702316  [29824128/29977280]\n",
            "loss: 13.589571  [29952128/29977280]\n",
            "Test Error: \n",
            " Accuracy: 19.7%, Avg loss: 13.431675 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 13.254612  [  128/29977280]\n",
            "loss: 13.661942  [128128/29977280]\n",
            "loss: 13.729284  [256128/29977280]\n",
            "loss: 13.428032  [384128/29977280]\n",
            "loss: 13.858397  [512128/29977280]\n",
            "loss: 13.530305  [640128/29977280]\n",
            "loss: 13.132339  [768128/29977280]\n",
            "loss: 13.366987  [896128/29977280]\n",
            "loss: 13.048623  [1024128/29977280]\n",
            "loss: 13.516135  [1152128/29977280]\n",
            "loss: 13.445758  [1280128/29977280]\n",
            "loss: 13.575511  [1408128/29977280]\n",
            "loss: 13.046214  [1536128/29977280]\n",
            "loss: 13.566338  [1664128/29977280]\n",
            "loss: 13.040076  [1792128/29977280]\n",
            "loss: 13.503023  [1920128/29977280]\n",
            "loss: 13.327841  [2048128/29977280]\n",
            "loss: 13.144251  [2176128/29977280]\n",
            "loss: 13.245685  [2304128/29977280]\n",
            "loss: 13.329266  [2432128/29977280]\n",
            "loss: 13.593261  [2560128/29977280]\n",
            "loss: 13.402085  [2688128/29977280]\n",
            "loss: 13.384281  [2816128/29977280]\n",
            "loss: 13.635912  [2944128/29977280]\n",
            "loss: 13.350629  [3072128/29977280]\n",
            "loss: 13.083040  [3200128/29977280]\n",
            "loss: 13.155686  [3328128/29977280]\n",
            "loss: 13.449622  [3456128/29977280]\n",
            "loss: 13.701051  [3584128/29977280]\n",
            "loss: 12.885407  [3712128/29977280]\n",
            "loss: 13.414841  [3840128/29977280]\n",
            "loss: 13.448303  [3968128/29977280]\n",
            "loss: 13.253370  [4096128/29977280]\n",
            "loss: 13.797451  [4224128/29977280]\n",
            "loss: 13.527834  [4352128/29977280]\n",
            "loss: 13.427608  [4480128/29977280]\n",
            "loss: 13.034346  [4608128/29977280]\n",
            "loss: 12.801377  [4736128/29977280]\n",
            "loss: 13.400563  [4864128/29977280]\n",
            "loss: 13.656906  [4992128/29977280]\n",
            "loss: 13.300160  [5120128/29977280]\n",
            "loss: 13.592850  [5248128/29977280]\n",
            "loss: 13.548519  [5376128/29977280]\n",
            "loss: 13.385077  [5504128/29977280]\n",
            "loss: 13.195341  [5632128/29977280]\n",
            "loss: 13.554616  [5760128/29977280]\n",
            "loss: 12.919021  [5888128/29977280]\n",
            "loss: 13.116522  [6016128/29977280]\n",
            "loss: 13.119894  [6144128/29977280]\n",
            "loss: 13.191025  [6272128/29977280]\n",
            "loss: 13.059963  [6400128/29977280]\n",
            "loss: 13.280934  [6528128/29977280]\n",
            "loss: 13.243391  [6656128/29977280]\n",
            "loss: 13.090431  [6784128/29977280]\n",
            "loss: 13.312424  [6912128/29977280]\n",
            "loss: 12.733603  [7040128/29977280]\n",
            "loss: 13.451481  [7168128/29977280]\n",
            "loss: 13.590300  [7296128/29977280]\n",
            "loss: 13.465266  [7424128/29977280]\n",
            "loss: 13.372086  [7552128/29977280]\n",
            "loss: 13.437509  [7680128/29977280]\n",
            "loss: 13.238039  [7808128/29977280]\n",
            "loss: 13.334365  [7936128/29977280]\n",
            "loss: 13.138426  [8064128/29977280]\n",
            "loss: 12.753000  [8192128/29977280]\n",
            "loss: 12.927076  [8320128/29977280]\n",
            "loss: 13.310041  [8448128/29977280]\n",
            "loss: 14.011361  [8576128/29977280]\n",
            "loss: 13.378859  [8704128/29977280]\n",
            "loss: 13.253360  [8832128/29977280]\n",
            "loss: 13.564230  [8960128/29977280]\n",
            "loss: 13.080318  [9088128/29977280]\n",
            "loss: 12.932897  [9216128/29977280]\n",
            "loss: 13.675641  [9344128/29977280]\n",
            "loss: 13.187319  [9472128/29977280]\n",
            "loss: 13.110497  [9600128/29977280]\n",
            "loss: 13.409588  [9728128/29977280]\n",
            "loss: 13.318722  [9856128/29977280]\n",
            "loss: 13.250858  [9984128/29977280]\n",
            "loss: 13.268602  [10112128/29977280]\n",
            "loss: 13.460175  [10240128/29977280]\n",
            "loss: 13.510618  [10368128/29977280]\n",
            "loss: 13.499230  [10496128/29977280]\n",
            "loss: 13.335726  [10624128/29977280]\n",
            "loss: 13.099740  [10752128/29977280]\n",
            "loss: 13.225669  [10880128/29977280]\n",
            "loss: 13.209553  [11008128/29977280]\n",
            "loss: 12.717175  [11136128/29977280]\n",
            "loss: 13.708491  [11264128/29977280]\n",
            "loss: 13.304652  [11392128/29977280]\n",
            "loss: 13.180820  [11520128/29977280]\n",
            "loss: 13.624058  [11648128/29977280]\n",
            "loss: 13.160769  [11776128/29977280]\n",
            "loss: 12.869212  [11904128/29977280]\n",
            "loss: 13.089160  [12032128/29977280]\n",
            "loss: 13.531055  [12160128/29977280]\n",
            "loss: 13.218754  [12288128/29977280]\n",
            "loss: 13.404554  [12416128/29977280]\n",
            "loss: 13.262136  [12544128/29977280]\n",
            "loss: 13.511400  [12672128/29977280]\n",
            "loss: 13.960248  [12800128/29977280]\n",
            "loss: 13.193485  [12928128/29977280]\n",
            "loss: 13.253615  [13056128/29977280]\n",
            "loss: 12.968098  [13184128/29977280]\n",
            "loss: 13.504461  [13312128/29977280]\n",
            "loss: 13.338083  [13440128/29977280]\n",
            "loss: 13.340588  [13568128/29977280]\n",
            "loss: 13.588307  [13696128/29977280]\n",
            "loss: 12.984950  [13824128/29977280]\n",
            "loss: 13.347954  [13952128/29977280]\n",
            "loss: 12.898687  [14080128/29977280]\n",
            "loss: 13.490063  [14208128/29977280]\n",
            "loss: 13.467878  [14336128/29977280]\n",
            "loss: 13.319128  [14464128/29977280]\n",
            "loss: 13.162617  [14592128/29977280]\n",
            "loss: 13.411362  [14720128/29977280]\n",
            "loss: 12.896980  [14848128/29977280]\n",
            "loss: 13.046673  [14976128/29977280]\n",
            "loss: 12.942951  [15104128/29977280]\n",
            "loss: 12.412675  [15232128/29977280]\n",
            "loss: 13.494902  [15360128/29977280]\n",
            "loss: 13.417453  [15488128/29977280]\n",
            "loss: 12.799926  [15616128/29977280]\n",
            "loss: 12.757342  [15744128/29977280]\n",
            "loss: 12.903073  [15872128/29977280]\n",
            "loss: 13.057919  [16000128/29977280]\n",
            "loss: 13.571442  [16128128/29977280]\n",
            "loss: 13.432035  [16256128/29977280]\n",
            "loss: 13.342557  [16384128/29977280]\n",
            "loss: 13.374131  [16512128/29977280]\n",
            "loss: 12.990479  [16640128/29977280]\n",
            "loss: 13.081826  [16768128/29977280]\n",
            "loss: 13.242367  [16896128/29977280]\n",
            "loss: 13.334648  [17024128/29977280]\n",
            "loss: 13.354361  [17152128/29977280]\n",
            "loss: 13.348013  [17280128/29977280]\n",
            "loss: 13.226786  [17408128/29977280]\n",
            "loss: 13.002275  [17536128/29977280]\n",
            "loss: 13.396848  [17664128/29977280]\n",
            "loss: 12.995628  [17792128/29977280]\n",
            "loss: 13.080118  [17920128/29977280]\n",
            "loss: 13.056778  [18048128/29977280]\n",
            "loss: 13.350419  [18176128/29977280]\n",
            "loss: 13.000720  [18304128/29977280]\n",
            "loss: 13.013220  [18432128/29977280]\n",
            "loss: 13.411427  [18560128/29977280]\n",
            "loss: 12.903377  [18688128/29977280]\n",
            "loss: 13.304189  [18816128/29977280]\n",
            "loss: 13.018696  [18944128/29977280]\n",
            "loss: 13.550301  [19072128/29977280]\n",
            "loss: 13.298924  [19200128/29977280]\n",
            "loss: 13.332207  [19328128/29977280]\n",
            "loss: 12.934261  [19456128/29977280]\n",
            "loss: 13.544637  [19584128/29977280]\n",
            "loss: 13.100567  [19712128/29977280]\n",
            "loss: 13.454498  [19840128/29977280]\n",
            "loss: 13.067116  [19968128/29977280]\n",
            "loss: 13.124148  [20096128/29977280]\n",
            "loss: 13.400848  [20224128/29977280]\n",
            "loss: 13.379823  [20352128/29977280]\n",
            "loss: 13.127392  [20480128/29977280]\n",
            "loss: 13.233943  [20608128/29977280]\n",
            "loss: 13.244862  [20736128/29977280]\n",
            "loss: 13.187438  [20864128/29977280]\n",
            "loss: 12.601974  [20992128/29977280]\n",
            "loss: 13.479225  [21120128/29977280]\n",
            "loss: 13.440937  [21248128/29977280]\n",
            "loss: 13.225029  [21376128/29977280]\n",
            "loss: 13.223790  [21504128/29977280]\n",
            "loss: 13.120353  [21632128/29977280]\n",
            "loss: 12.929163  [21760128/29977280]\n",
            "loss: 13.254945  [21888128/29977280]\n",
            "loss: 13.025920  [22016128/29977280]\n",
            "loss: 12.656881  [22144128/29977280]\n",
            "loss: 13.216811  [22272128/29977280]\n",
            "loss: 13.242882  [22400128/29977280]\n",
            "loss: 13.004070  [22528128/29977280]\n",
            "loss: 13.103931  [22656128/29977280]\n",
            "loss: 12.938793  [22784128/29977280]\n",
            "loss: 12.940899  [22912128/29977280]\n",
            "loss: 13.468147  [23040128/29977280]\n",
            "loss: 13.215111  [23168128/29977280]\n",
            "loss: 12.980704  [23296128/29977280]\n",
            "loss: 13.437510  [23424128/29977280]\n",
            "loss: 13.184101  [23552128/29977280]\n",
            "loss: 13.337487  [23680128/29977280]\n",
            "loss: 12.854344  [23808128/29977280]\n",
            "loss: 12.953647  [23936128/29977280]\n",
            "loss: 13.465226  [24064128/29977280]\n",
            "loss: 12.977798  [24192128/29977280]\n",
            "loss: 13.023520  [24320128/29977280]\n",
            "loss: 13.098079  [24448128/29977280]\n",
            "loss: 13.202611  [24576128/29977280]\n",
            "loss: 12.930640  [24704128/29977280]\n",
            "loss: 13.183544  [24832128/29977280]\n",
            "loss: 13.249208  [24960128/29977280]\n",
            "loss: 13.366398  [25088128/29977280]\n",
            "loss: 13.035705  [25216128/29977280]\n",
            "loss: 13.054374  [25344128/29977280]\n",
            "loss: 13.193137  [25472128/29977280]\n",
            "loss: 13.162151  [25600128/29977280]\n",
            "loss: 13.197911  [25728128/29977280]\n",
            "loss: 13.151861  [25856128/29977280]\n",
            "loss: 12.410229  [25984128/29977280]\n",
            "loss: 13.110794  [26112128/29977280]\n",
            "loss: 13.049265  [26240128/29977280]\n",
            "loss: 13.133775  [26368128/29977280]\n",
            "loss: 13.381385  [26496128/29977280]\n",
            "loss: 13.226254  [26624128/29977280]\n",
            "loss: 13.681222  [26752128/29977280]\n",
            "loss: 13.057605  [26880128/29977280]\n",
            "loss: 13.218297  [27008128/29977280]\n",
            "loss: 12.973713  [27136128/29977280]\n",
            "loss: 13.186023  [27264128/29977280]\n",
            "loss: 13.414112  [27392128/29977280]\n",
            "loss: 12.771368  [27520128/29977280]\n",
            "loss: 12.869505  [27648128/29977280]\n",
            "loss: 13.137598  [27776128/29977280]\n",
            "loss: 13.398385  [27904128/29977280]\n",
            "loss: 13.498747  [28032128/29977280]\n",
            "loss: 13.195181  [28160128/29977280]\n",
            "loss: 13.020142  [28288128/29977280]\n",
            "loss: 12.785702  [28416128/29977280]\n",
            "loss: 12.762115  [28544128/29977280]\n",
            "loss: 12.945479  [28672128/29977280]\n",
            "loss: 12.921693  [28800128/29977280]\n",
            "loss: 12.973021  [28928128/29977280]\n",
            "loss: 13.248531  [29056128/29977280]\n",
            "loss: 13.532976  [29184128/29977280]\n",
            "loss: 13.228757  [29312128/29977280]\n",
            "loss: 12.881706  [29440128/29977280]\n",
            "loss: 13.399742  [29568128/29977280]\n",
            "loss: 13.310840  [29696128/29977280]\n",
            "loss: 13.001308  [29824128/29977280]\n",
            "loss: 13.134067  [29952128/29977280]\n",
            "Test Error: \n",
            " Accuracy: 20.8%, Avg loss: 13.145944 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 12.876961  [  128/29977280]\n",
            "loss: 13.387637  [128128/29977280]\n",
            "loss: 13.161448  [256128/29977280]\n",
            "loss: 13.065113  [384128/29977280]\n",
            "loss: 13.242758  [512128/29977280]\n",
            "loss: 13.034677  [640128/29977280]\n",
            "loss: 13.336092  [768128/29977280]\n",
            "loss: 12.768873  [896128/29977280]\n",
            "loss: 13.168663  [1024128/29977280]\n",
            "loss: 13.468904  [1152128/29977280]\n",
            "loss: 13.719243  [1280128/29977280]\n",
            "loss: 13.209647  [1408128/29977280]\n",
            "loss: 13.130423  [1536128/29977280]\n",
            "loss: 12.952710  [1664128/29977280]\n",
            "loss: 13.187851  [1792128/29977280]\n",
            "loss: 13.576043  [1920128/29977280]\n",
            "loss: 13.078163  [2048128/29977280]\n",
            "loss: 13.348731  [2176128/29977280]\n",
            "loss: 13.013151  [2304128/29977280]\n",
            "loss: 12.816637  [2432128/29977280]\n",
            "loss: 12.989496  [2560128/29977280]\n",
            "loss: 13.100893  [2688128/29977280]\n",
            "loss: 13.496111  [2816128/29977280]\n",
            "loss: 13.050333  [2944128/29977280]\n",
            "loss: 13.185788  [3072128/29977280]\n",
            "loss: 13.111587  [3200128/29977280]\n",
            "loss: 13.177772  [3328128/29977280]\n",
            "loss: 13.168326  [3456128/29977280]\n",
            "loss: 13.513491  [3584128/29977280]\n",
            "loss: 13.362962  [3712128/29977280]\n",
            "loss: 13.232830  [3840128/29977280]\n",
            "loss: 13.317612  [3968128/29977280]\n",
            "loss: 13.269018  [4096128/29977280]\n",
            "loss: 12.991771  [4224128/29977280]\n",
            "loss: 13.054286  [4352128/29977280]\n",
            "loss: 13.219679  [4480128/29977280]\n",
            "loss: 13.198559  [4608128/29977280]\n",
            "loss: 12.851226  [4736128/29977280]\n",
            "loss: 12.867148  [4864128/29977280]\n",
            "loss: 13.327978  [4992128/29977280]\n",
            "loss: 12.988846  [5120128/29977280]\n",
            "loss: 13.229673  [5248128/29977280]\n",
            "loss: 13.196877  [5376128/29977280]\n",
            "loss: 12.826025  [5504128/29977280]\n",
            "loss: 12.941542  [5632128/29977280]\n",
            "loss: 12.888155  [5760128/29977280]\n",
            "loss: 12.942360  [5888128/29977280]\n",
            "loss: 12.855210  [6016128/29977280]\n",
            "loss: 13.418240  [6144128/29977280]\n",
            "loss: 13.280966  [6272128/29977280]\n",
            "loss: 13.129995  [6400128/29977280]\n",
            "loss: 13.373416  [6528128/29977280]\n",
            "loss: 13.027843  [6656128/29977280]\n",
            "loss: 13.072929  [6784128/29977280]\n",
            "loss: 12.792397  [6912128/29977280]\n",
            "loss: 12.951974  [7040128/29977280]\n",
            "loss: 13.427813  [7168128/29977280]\n",
            "loss: 13.168685  [7296128/29977280]\n",
            "loss: 13.551907  [7424128/29977280]\n",
            "loss: 13.062765  [7552128/29977280]\n",
            "loss: 13.745226  [7680128/29977280]\n",
            "loss: 12.908726  [7808128/29977280]\n",
            "loss: 13.203561  [7936128/29977280]\n",
            "loss: 12.809498  [8064128/29977280]\n",
            "loss: 13.036434  [8192128/29977280]\n",
            "loss: 12.913918  [8320128/29977280]\n",
            "loss: 13.043556  [8448128/29977280]\n",
            "loss: 12.790388  [8576128/29977280]\n",
            "loss: 13.065558  [8704128/29977280]\n",
            "loss: 13.304705  [8832128/29977280]\n",
            "loss: 13.256136  [8960128/29977280]\n",
            "loss: 12.861101  [9088128/29977280]\n",
            "loss: 13.139678  [9216128/29977280]\n",
            "loss: 13.212029  [9344128/29977280]\n",
            "loss: 13.448708  [9472128/29977280]\n",
            "loss: 12.984504  [9600128/29977280]\n",
            "loss: 12.794044  [9728128/29977280]\n",
            "loss: 13.155741  [9856128/29977280]\n",
            "loss: 13.073296  [9984128/29977280]\n",
            "loss: 13.007467  [10112128/29977280]\n",
            "loss: 12.670006  [10240128/29977280]\n",
            "loss: 12.926893  [10368128/29977280]\n",
            "loss: 13.167110  [10496128/29977280]\n",
            "loss: 13.353097  [10624128/29977280]\n",
            "loss: 13.415975  [10752128/29977280]\n",
            "loss: 13.288985  [10880128/29977280]\n",
            "loss: 12.643038  [11008128/29977280]\n",
            "loss: 13.505550  [11136128/29977280]\n",
            "loss: 12.887094  [11264128/29977280]\n",
            "loss: 13.357078  [11392128/29977280]\n",
            "loss: 13.170141  [11520128/29977280]\n",
            "loss: 12.609058  [11648128/29977280]\n",
            "loss: 13.257657  [11776128/29977280]\n",
            "loss: 13.196781  [11904128/29977280]\n",
            "loss: 13.212637  [12032128/29977280]\n",
            "loss: 13.186249  [12160128/29977280]\n",
            "loss: 13.271332  [12288128/29977280]\n",
            "loss: 13.023058  [12416128/29977280]\n",
            "loss: 13.368040  [12544128/29977280]\n",
            "loss: 12.913331  [12672128/29977280]\n",
            "loss: 13.278172  [12800128/29977280]\n",
            "loss: 12.940693  [12928128/29977280]\n",
            "loss: 12.933032  [13056128/29977280]\n",
            "loss: 12.820210  [13184128/29977280]\n",
            "loss: 12.737684  [13312128/29977280]\n",
            "loss: 13.345825  [13440128/29977280]\n",
            "loss: 13.077558  [13568128/29977280]\n",
            "loss: 12.806358  [13696128/29977280]\n",
            "loss: 13.172792  [13824128/29977280]\n",
            "loss: 13.050339  [13952128/29977280]\n",
            "loss: 13.269297  [14080128/29977280]\n",
            "loss: 12.935214  [14208128/29977280]\n",
            "loss: 13.204284  [14336128/29977280]\n",
            "loss: 13.345963  [14464128/29977280]\n",
            "loss: 12.801341  [14592128/29977280]\n",
            "loss: 13.379663  [14720128/29977280]\n",
            "loss: 13.151965  [14848128/29977280]\n",
            "loss: 13.136226  [14976128/29977280]\n",
            "loss: 13.621816  [15104128/29977280]\n",
            "loss: 12.935811  [15232128/29977280]\n",
            "loss: 13.123062  [15360128/29977280]\n",
            "loss: 13.086380  [15488128/29977280]\n",
            "loss: 13.023127  [15616128/29977280]\n",
            "loss: 13.227369  [15744128/29977280]\n",
            "loss: 13.176523  [15872128/29977280]\n",
            "loss: 13.184915  [16000128/29977280]\n",
            "loss: 13.450882  [16128128/29977280]\n",
            "loss: 13.335388  [16256128/29977280]\n",
            "loss: 13.548723  [16384128/29977280]\n",
            "loss: 12.927338  [16512128/29977280]\n",
            "loss: 13.042594  [16640128/29977280]\n",
            "loss: 13.646380  [16768128/29977280]\n",
            "loss: 13.010151  [16896128/29977280]\n",
            "loss: 13.345964  [17024128/29977280]\n",
            "loss: 12.853294  [17152128/29977280]\n",
            "loss: 13.014596  [17280128/29977280]\n",
            "loss: 12.833393  [17408128/29977280]\n",
            "loss: 13.099837  [17536128/29977280]\n",
            "loss: 13.059311  [17664128/29977280]\n",
            "loss: 13.105257  [17792128/29977280]\n",
            "loss: 13.306511  [17920128/29977280]\n",
            "loss: 13.042502  [18048128/29977280]\n",
            "loss: 12.860928  [18176128/29977280]\n",
            "loss: 12.592482  [18304128/29977280]\n",
            "loss: 13.355620  [18432128/29977280]\n",
            "loss: 12.966322  [18560128/29977280]\n",
            "loss: 13.143586  [18688128/29977280]\n",
            "loss: 12.635900  [18816128/29977280]\n",
            "loss: 13.059024  [18944128/29977280]\n",
            "loss: 13.086258  [19072128/29977280]\n",
            "loss: 13.230074  [19200128/29977280]\n",
            "loss: 12.806697  [19328128/29977280]\n",
            "loss: 12.565238  [19456128/29977280]\n",
            "loss: 13.147287  [19584128/29977280]\n",
            "loss: 13.030238  [19712128/29977280]\n",
            "loss: 13.068440  [19840128/29977280]\n",
            "loss: 13.410128  [19968128/29977280]\n",
            "loss: 13.293880  [20096128/29977280]\n",
            "loss: 12.935440  [20224128/29977280]\n",
            "loss: 12.769384  [20352128/29977280]\n",
            "loss: 13.075616  [20480128/29977280]\n",
            "loss: 12.961865  [20608128/29977280]\n",
            "loss: 13.079447  [20736128/29977280]\n",
            "loss: 13.421824  [20864128/29977280]\n",
            "loss: 12.446021  [20992128/29977280]\n",
            "loss: 12.822504  [21120128/29977280]\n",
            "loss: 13.368281  [21248128/29977280]\n",
            "loss: 13.335515  [21376128/29977280]\n",
            "loss: 13.388988  [21504128/29977280]\n",
            "loss: 13.154426  [21632128/29977280]\n",
            "loss: 13.293542  [21760128/29977280]\n",
            "loss: 13.011593  [21888128/29977280]\n",
            "loss: 13.188510  [22016128/29977280]\n",
            "loss: 12.873221  [22144128/29977280]\n",
            "loss: 12.723022  [22272128/29977280]\n",
            "loss: 13.133977  [22400128/29977280]\n",
            "loss: 13.310049  [22528128/29977280]\n",
            "loss: 13.296200  [22656128/29977280]\n",
            "loss: 12.902091  [22784128/29977280]\n",
            "loss: 13.536996  [22912128/29977280]\n",
            "loss: 13.141333  [23040128/29977280]\n",
            "loss: 13.027423  [23168128/29977280]\n",
            "loss: 12.858582  [23296128/29977280]\n",
            "loss: 13.147397  [23424128/29977280]\n",
            "loss: 13.163925  [23552128/29977280]\n",
            "loss: 13.327598  [23680128/29977280]\n",
            "loss: 12.754247  [23808128/29977280]\n",
            "loss: 13.539165  [23936128/29977280]\n",
            "loss: 13.400393  [24064128/29977280]\n",
            "loss: 13.281981  [24192128/29977280]\n",
            "loss: 12.928717  [24320128/29977280]\n",
            "loss: 13.302913  [24448128/29977280]\n",
            "loss: 12.986933  [24576128/29977280]\n",
            "loss: 13.005262  [24704128/29977280]\n",
            "loss: 13.334915  [24832128/29977280]\n",
            "loss: 12.560995  [24960128/29977280]\n",
            "loss: 13.073765  [25088128/29977280]\n",
            "loss: 13.273204  [25216128/29977280]\n",
            "loss: 12.963428  [25344128/29977280]\n",
            "loss: 13.128194  [25472128/29977280]\n",
            "loss: 13.769484  [25600128/29977280]\n",
            "loss: 13.111113  [25728128/29977280]\n",
            "loss: 12.904337  [25856128/29977280]\n",
            "loss: 12.986727  [25984128/29977280]\n",
            "loss: 12.986694  [26112128/29977280]\n",
            "loss: 13.140230  [26240128/29977280]\n",
            "loss: 13.320045  [26368128/29977280]\n",
            "loss: 13.603521  [26496128/29977280]\n",
            "loss: 12.940399  [26624128/29977280]\n",
            "loss: 13.028902  [26752128/29977280]\n",
            "loss: 12.464767  [26880128/29977280]\n",
            "loss: 12.928094  [27008128/29977280]\n",
            "loss: 13.101212  [27136128/29977280]\n",
            "loss: 13.090676  [27264128/29977280]\n",
            "loss: 13.043032  [27392128/29977280]\n",
            "loss: 13.244016  [27520128/29977280]\n",
            "loss: 13.112069  [27648128/29977280]\n",
            "loss: 13.199571  [27776128/29977280]\n",
            "loss: 12.837391  [27904128/29977280]\n",
            "loss: 13.072380  [28032128/29977280]\n",
            "loss: 13.011230  [28160128/29977280]\n",
            "loss: 13.529065  [28288128/29977280]\n",
            "loss: 12.830932  [28416128/29977280]\n",
            "loss: 13.099937  [28544128/29977280]\n",
            "loss: 13.198567  [28672128/29977280]\n",
            "loss: 13.324387  [28800128/29977280]\n",
            "loss: 12.670379  [28928128/29977280]\n",
            "loss: 13.016233  [29056128/29977280]\n",
            "loss: 13.477020  [29184128/29977280]\n",
            "loss: 13.036146  [29312128/29977280]\n",
            "loss: 12.791451  [29440128/29977280]\n",
            "loss: 13.137715  [29568128/29977280]\n",
            "loss: 13.324851  [29696128/29977280]\n",
            "loss: 13.189075  [29824128/29977280]\n",
            "loss: 12.721285  [29952128/29977280]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input_for_prediction(masked_word):\n",
        "    \"\"\"\n",
        "    Encode the masked word.\n",
        "\n",
        "    Parameters:\n",
        "    - masked_word (str): The word with masked characters.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Encoded input tensor for prediction.\n",
        "    \"\"\"\n",
        "    # Create a character-to-index mapping and an underscore placeholder\n",
        "    char_to_index, _ = create_char_mapping()\n",
        "\n",
        "    # Encode the masked word using the char_to_index mapping\n",
        "    input_data = [encode_input(masked_word)]\n",
        "    input_tensor = torch.tensor(input_data, dtype=torch.long)\n",
        "    return input_tensor\n",
        "\n",
        "\n",
        "def extract_ngrams(word):\n",
        "    \"\"\"\n",
        "    Make all possible n-grams from the word with at least one underscore.\n",
        "\n",
        "    Parameters:\n",
        "    - word (str): The input word.\n",
        "\n",
        "    Returns:\n",
        "    - list: List of unique n-grams.\n",
        "    \"\"\"\n",
        "    ngrams = set()\n",
        "\n",
        "    # Iterate over different n-gram lengths (2, 3, 4, 5, 6)\n",
        "    for n in range(2, 7):\n",
        "        # Extract n-grams from the word\n",
        "        for i in range(len(word) - n + 1):\n",
        "            ngram = word[i:i + n]\n",
        "\n",
        "            # Check if the n-gram contains at least one alphabet and one underscore\n",
        "            if any(char.isalpha() for char in ngram) and '_' in ngram:\n",
        "                # If the n-gram is shorter than 6, pad it with asterisks\n",
        "                ngram = ngram.ljust(6, '*')\n",
        "\n",
        "                # Ensure the n-gram is of length 6\n",
        "                ngram = ngram[:6]\n",
        "\n",
        "                # Add the n-gram to the set\n",
        "                ngrams.add(ngram)\n",
        "\n",
        "    return list(ngrams)\n",
        "\n",
        "def encode_ngram(ngram, char_to_index):\n",
        "    \"\"\"\n",
        "    Encode a given n-gram using a character-to-index mapping.\n",
        "\n",
        "    Parameters:\n",
        "    - ngram (str): The input n-gram.\n",
        "    - char_to_index (dict): Character-to-index mapping.\n",
        "\n",
        "    Returns:\n",
        "    - list: Encoded n-gram.\n",
        "    \"\"\"\n",
        "    # Ensure the n-gram is of length 6\n",
        "    ngram = ngram[:6]\n",
        "\n",
        "    # Encode each character in the n-gram using char_to_index mapping\n",
        "    encoded_ngram = [char_to_index[char] for char in ngram]\n",
        "\n",
        "    return encoded_ngram\n",
        "\n",
        "def get_sorted_letters(new_dictionary, guessed_letters):\n",
        "    \"\"\"\n",
        "    Get sorted letters based on their frequency in the new dictionary.\n",
        "\n",
        "    Parameters:\n",
        "    - new_dictionary (list): List of possible words.\n",
        "    - guessed_letters (list): List of guessed letters.\n",
        "\n",
        "    Returns:\n",
        "    - list: Sorted letters based on frequency, excluding guessed letters.\n",
        "    \"\"\"\n",
        "    full_dict_string = \"\".join(new_dictionary)\n",
        "\n",
        "    # Count the occurrences of each letter\n",
        "    c = collections.Counter(full_dict_string)\n",
        "\n",
        "    sorted_letter_count = c.most_common()\n",
        "\n",
        "    # Filter out guessed letters\n",
        "    remaining_sorted_letters = [item for item in sorted_letter_count if item[0] not in guessed_letters]\n",
        "\n",
        "    return remaining_sorted_letters\n",
        "\n",
        "def func(new_dictionary):\n",
        "    \"\"\"\n",
        "    Count the occurrences of each letter in the new dictionary.\n",
        "\n",
        "    Parameters:\n",
        "    - new_dictionary (list): List of possible words.\n",
        "\n",
        "    Returns:\n",
        "    - collections.Counter: Count of occurrences for each letter.\n",
        "    \"\"\"\n",
        "    dictx = collections.Counter()\n",
        "    for words in new_dictionary:\n",
        "        temp = collections.Counter(words)\n",
        "        for i in temp:\n",
        "            temp[i] = 1\n",
        "            dictx = dictx + temp\n",
        "    return dictx"
      ],
      "metadata": {
        "id": "U7qFgX38Djy8"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HangmanAPI(object):\n",
        "    def __init__(self, access_token=None, session=None, timeout=None):\n",
        "        self.hangman_url = self.determine_hangman_url()\n",
        "        self.access_token = access_token\n",
        "        self.session = session or requests.Session()\n",
        "        self.timeout = timeout\n",
        "        self.guessed_letters = []\n",
        "        full_dictionary_location = \"/content/words_250000_train.txt\"\n",
        "        self.full_dictionary = self.build_dictionary(full_dictionary_location)\n",
        "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
        "        self.current_dictionary = []\n",
        "        self.tries_remains  = 6\n",
        "        self.model = NeuralNetwork()\n",
        "        self.model.load_state_dict(torch.load(\"/content/lstm_ngram_2.pt\"))\n",
        "        self.ngram_used = set()\n",
        "        self.LSTM_guess = 0\n",
        "\n",
        "    @staticmethod\n",
        "    def determine_hangman_url():\n",
        "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
        "\n",
        "        data = {link: 0 for link in links}\n",
        "\n",
        "        for link in links:\n",
        "\n",
        "            requests.get(link)\n",
        "\n",
        "            for i in range(10):\n",
        "                s = time.time()\n",
        "                requests.get(link)\n",
        "                data[link] = time.time() - s\n",
        "\n",
        "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
        "        link += '/trexsim/hangman'\n",
        "        return link\n",
        "\n",
        "\n",
        "    def predicted_letter_lstm(self, masked_word):\n",
        "        \"\"\"\n",
        "        Predict the next letter using the LSTM model based on the masked word.\n",
        "\n",
        "        Parameters:\n",
        "        - masked_word (str): The word with masked characters.\n",
        "\n",
        "        Returns:\n",
        "        - list: Predicted letters sorted by probability.\n",
        "        \"\"\"\n",
        "        # Create character mappings\n",
        "        char_to_index, int_to_char = create_char_mapping()\n",
        "\n",
        "        # Extract unique n-grams with at least one alphabet\n",
        "        ngrams = extract_ngrams(masked_word)\n",
        "\n",
        "        # Initialize an empty dictionary to store accumulated probabilities\n",
        "        accumulated_probabilities = {}\n",
        "\n",
        "        # Traverse over n-grams\n",
        "        for ngram in ngrams:\n",
        "            # Encode the n-gram\n",
        "            input_tensor_for_prediction = encode_ngram(ngram, char_to_index)\n",
        "\n",
        "            # Convert to tensor\n",
        "            input_tensor = torch.tensor(input_tensor_for_prediction, dtype=torch.long)\n",
        "            input_tensor = input_tensor.view(1, -1)\n",
        "\n",
        "            # Ensure the model is in evaluation mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Make predictions\n",
        "            with torch.no_grad():\n",
        "                output = self.model(input_tensor)\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probabilities = torch.softmax(output, dim=1).numpy()\n",
        "\n",
        "            # Process the probabilities using int_to_char\n",
        "            probabilities_list = [(int_to_char[i], prob) for i, prob in enumerate(probabilities[0])]\n",
        "\n",
        "            alphabet_count = sum(1 for char in ngram if char.isalpha())\n",
        "\n",
        "            # Give more weight to the new built n-gram (new information)\n",
        "            if ngram not in self.ngram_used:\n",
        "                self.ngram_used.add(ngram)\n",
        "                weight = 2\n",
        "            else:\n",
        "                weight = 1\n",
        "\n",
        "            # Accumulate probabilities for each alphabet\n",
        "            for char, prob in probabilities_list:\n",
        "                accumulated_probabilities[char] = accumulated_probabilities.get(char, 0) + prob * weight\n",
        "\n",
        "        # Convert the accumulated probabilities to a list of tuples\n",
        "        final_accumulated_list = list(accumulated_probabilities.items())\n",
        "\n",
        "        sorted_probabilities_list = sorted(final_accumulated_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        sorted_letters_list = [pred for pred, _ in sorted_probabilities_list]\n",
        "\n",
        "        return sorted_letters_list\n",
        "\n",
        "\n",
        "    def guess(self, word):\n",
        "        # word input example: \"_ p p _ e \"\n",
        "\n",
        "        # clean the word so that we strip away the space characters\n",
        "        # replace \"_\" with \".\" as \".\" indicates any character in regular expressions\n",
        "        clean_word = word[::2].replace(\"_\",\".\")\n",
        "\n",
        "        # find length of passed word\n",
        "        len_word = len(clean_word)\n",
        "\n",
        "        # remaing spaces\n",
        "        remaining_spaces = clean_word.count('.')\n",
        "\n",
        "        # grab current dictionary of possible words from self object, initialize new possible words dictionary to empty\n",
        "        current_dictionary = self.current_dictionary\n",
        "        new_dictionary = []\n",
        "\n",
        "        # iterate through all of the words in the old plausible dictionary\n",
        "        for dict_word in current_dictionary:\n",
        "            # continue if the word is not of the appropriate length\n",
        "            if len(dict_word) != len_word:\n",
        "                continue\n",
        "\n",
        "            # if dictionary word is a possible match then add it to the current dictionary\n",
        "            if re.match(clean_word,dict_word):\n",
        "                new_dictionary.append(dict_word)\n",
        "\n",
        "        # overwrite old possible words dictionary with updated version\n",
        "        self.current_dictionary = new_dictionary\n",
        "\n",
        "        # start the guess letter\n",
        "        guess_letter = '!'\n",
        "\n",
        "        # if we have not yet guessed at least 2, start with most common letters,\n",
        "        # in the dictionary of the words with the same length\n",
        "        if (len_word - remaining_spaces) < 2:\n",
        "            full_dict_string = \"\".join(new_dictionary)\n",
        "            # return most frequently occurring letter in all possible words that hasn't been guessed yet\n",
        "            c = collections.Counter(full_dict_string)\n",
        "            sorted_letter_count = c.most_common()\n",
        "            for letter,_ in sorted_letter_count:\n",
        "                if letter not in self.guessed_letters:\n",
        "                    guess_letter = letter\n",
        "                    break\n",
        "\n",
        "        remaining_sorted_letters = get_sorted_letters(new_dictionary, self.guessed_letters)\n",
        "\n",
        "        # now we have at least two letters, use LSTM:\n",
        "        if guess_letter == '!':\n",
        "          # if the last three guest by LSTM is wrong, don't use it\n",
        "          if self.LSTM_guess > 2 and all(letter not in clean_word for letter in self.guessed_letters[-3:]):\n",
        "            self.LSTM_guess = 0\n",
        "          # otherwise use LSTM\n",
        "          else:\n",
        "            predict_letters = self.predicted_letter_lstm(word[::2])\n",
        "            for letter in predict_letters:\n",
        "                # check if the prediction is alphabet and it is not already suggested\n",
        "                if letter.isalpha() and letter not in self.guessed_letters:\n",
        "                    guess_letter = letter\n",
        "                    self.LSTM_guess += 1\n",
        "                    break\n",
        "\n",
        "        # if there was no match: based on words with the same pattern:\n",
        "        if guess_letter == '!' or guess_letter == '_':\n",
        "            # return most frequently occurring letter in all possible words that hasn't been guessed yet\n",
        "            c = func(new_dictionary)\n",
        "            sorted_letter_count = c.most_common()\n",
        "            for letter,_ in sorted_letter_count:\n",
        "                if letter not in self.guessed_letters:\n",
        "                    guess_letter = letter\n",
        "                    break\n",
        "\n",
        "        # if no word matches in training dictionary, default back to ordering of full dictionary\n",
        "        if guess_letter == '!' or guess_letter == '_':\n",
        "            sorted_letter_count = self.full_dictionary_common_letter_sorted\n",
        "            for letter,_ in sorted_letter_count:\n",
        "                if letter not in self.guessed_letters:\n",
        "                    guess_letter = letter\n",
        "                    break\n",
        "\n",
        "        return guess_letter\n",
        "\n",
        "    ##########################################################\n",
        "    # You'll likely not need to modify any of the code below #\n",
        "    ##########################################################\n",
        "\n",
        "    def build_dictionary(self, dictionary_file_location):\n",
        "        text_file = open(dictionary_file_location,\"r\")\n",
        "        full_dictionary = text_file.read().splitlines()\n",
        "        text_file.close()\n",
        "        return full_dictionary\n",
        "\n",
        "    def start_game(self, practice=True, verbose=True):\n",
        "        # reset guessed letters to empty set and current plausible dictionary to the full dictionary\n",
        "        self.guessed_letters = []\n",
        "        self.current_dictionary = self.full_dictionary\n",
        "\n",
        "        response = self.request(\"/new_game\", {\"practice\":practice})\n",
        "        if response.get('status')==\"approved\":\n",
        "            game_id = response.get('game_id')\n",
        "            word = response.get('word')\n",
        "            self.tries_remains = response.get('tries_remains')\n",
        "            if verbose:\n",
        "                print(\"Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, self.tries_remains, word))\n",
        "            while self.tries_remains > 0:\n",
        "                # get guessed letter from user code\n",
        "                guess_letter = self.guess(word)\n",
        "\n",
        "                # append guessed letter to guessed letters field in hangman object\n",
        "                self.guessed_letters.append(guess_letter)\n",
        "                if verbose:\n",
        "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
        "\n",
        "                try:\n",
        "                    res = self.request(\"/guess_letter\", {\"request\":\"guess_letter\", \"game_id\":game_id, \"letter\":guess_letter})\n",
        "                except HangmanAPIError:\n",
        "                    print('HangmanAPIError exception caught on request.')\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    print('Other exception caught on request.')\n",
        "                    raise e\n",
        "\n",
        "                if verbose:\n",
        "                    print(\"Sever response: {0}\".format(res))\n",
        "                status = res.get('status')\n",
        "                self.tries_remains = res.get('tries_remains')\n",
        "                if status==\"success\":\n",
        "                    if verbose:\n",
        "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
        "                    return True\n",
        "                elif status==\"failed\":\n",
        "                    reason = res.get('reason', '# of tries exceeded!')\n",
        "                    if verbose:\n",
        "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
        "                    return False\n",
        "                elif status==\"ongoing\":\n",
        "                    word = res.get('word')\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"Failed to start a new game\")\n",
        "        return status==\"success\"\n",
        "\n",
        "    def my_status(self):\n",
        "        return self.request(\"/my_status\", {})\n",
        "\n",
        "    def request(\n",
        "            self, path, args=None, post_args=None, method=None):\n",
        "        if args is None:\n",
        "            args = dict()\n",
        "        if post_args is not None:\n",
        "            method = \"POST\"\n",
        "\n",
        "        # Add `access_token` to post_args or args if it has not already been\n",
        "        # included.\n",
        "        if self.access_token:\n",
        "            # If post_args exists, we assume that args either does not exists\n",
        "            # or it does not need `access_token`.\n",
        "            if post_args and \"access_token\" not in post_args:\n",
        "                post_args[\"access_token\"] = self.access_token\n",
        "            elif \"access_token\" not in args:\n",
        "                args[\"access_token\"] = self.access_token\n",
        "\n",
        "        time.sleep(0.2)\n",
        "\n",
        "        num_retry, time_sleep = 50, 2\n",
        "        for it in range(num_retry):\n",
        "            try:\n",
        "                response = self.session.request(\n",
        "                    method or \"GET\",\n",
        "                    self.hangman_url + path,\n",
        "                    timeout=self.timeout,\n",
        "                    params=args,\n",
        "                    data=post_args,\n",
        "                    verify=False\n",
        "                )\n",
        "                break\n",
        "            except requests.HTTPError as e:\n",
        "                response = json.loads(e.read())\n",
        "                raise HangmanAPIError(response)\n",
        "            except requests.exceptions.SSLError as e:\n",
        "                if it + 1 == num_retry:\n",
        "                    raise\n",
        "                time.sleep(time_sleep)\n",
        "\n",
        "        headers = response.headers\n",
        "        if 'json' in headers['content-type']:\n",
        "            result = response.json()\n",
        "        elif \"access_token\" in parse_qs(response.text):\n",
        "            query_str = parse_qs(response.text)\n",
        "            if \"access_token\" in query_str:\n",
        "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
        "                if \"expires\" in query_str:\n",
        "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
        "            else:\n",
        "                raise HangmanAPIError(response.json())\n",
        "        else:\n",
        "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
        "\n",
        "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
        "            raise HangmanAPIError(result)\n",
        "        return result\n",
        "\n",
        "class HangmanAPIError(Exception):\n",
        "    def __init__(self, result):\n",
        "        self.result = result\n",
        "        self.code = None\n",
        "        try:\n",
        "            self.type = result[\"error_code\"]\n",
        "        except (KeyError, TypeError):\n",
        "            self.type = \"\"\n",
        "\n",
        "        try:\n",
        "            self.message = result[\"error_description\"]\n",
        "        except (KeyError, TypeError):\n",
        "            try:\n",
        "                self.message = result[\"error\"][\"message\"]\n",
        "                self.code = result[\"error\"].get(\"code\")\n",
        "                if not self.type:\n",
        "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
        "            except (KeyError, TypeError):\n",
        "                try:\n",
        "                    self.message = result[\"error_msg\"]\n",
        "                except (KeyError, TypeError):\n",
        "                    self.message = result\n",
        "\n",
        "        Exception.__init__(self, self.message)"
      ],
      "metadata": {
        "id": "dchQERaVHcpe"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api = HangmanAPI(access_token=\"8639b13cb6b189aee1f97ac663b526\", timeout=2000)"
      ],
      "metadata": {
        "id": "ogqo0N4EHfNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e79bb868-2266-4041-dca0-2d722dca409d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range (100):\n",
        "    api.start_game(practice=1,verbose=False)"
      ],
      "metadata": {
        "id": "A5U5lGOofGgb"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
        "practice_success_rate = total_practice_successes / total_practice_runs\n",
        "print('run %d practice games out of an allotted 100,000. practice success rate so far = %.3f' % (total_practice_runs, practice_success_rate))"
      ],
      "metadata": {
        "id": "-xqfHnqeHgdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6587edaa-73ab-4a93-e75a-3c43c31bba23"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run 1233 practice games out of an allotted 100,000. practice success rate so far = 0.174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Playing recorded games:**\n",
        "Please finalize your code prior to running the cell below. Once this code executes once successfully your submission will be finalized. Our system will not allow you to rerun any additional games.\n",
        "\n",
        "Please note that it is expected that after you successfully run this block of code that subsequent runs will result in the error message \"Your account has been deactivated\".\n",
        "\n",
        "Once you've run this section of the code your submission is complete. Please send us your source code via email."
      ],
      "metadata": {
        "id": "qAR8zhjmcNAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "    print('Playing ', i, ' th game')\n",
        "    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\n",
        "    api.start_game(practice=0, verbose=False)\n",
        "\n",
        "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
        "    time.sleep(0.5)"
      ],
      "metadata": {
        "id": "8L84nx0Smbm3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8acdf1e7-918a-4298-c59a-1c58a0f3bae9"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Playing  0  th game\n",
            "Playing  1  th game\n",
            "Playing  2  th game\n",
            "Playing  3  th game\n",
            "Playing  4  th game\n",
            "Playing  5  th game\n",
            "Playing  6  th game\n",
            "Playing  7  th game\n",
            "Playing  8  th game\n",
            "Playing  9  th game\n",
            "Playing  10  th game\n",
            "Playing  11  th game\n",
            "Playing  12  th game\n",
            "Playing  13  th game\n",
            "Playing  14  th game\n",
            "Playing  15  th game\n",
            "Playing  16  th game\n",
            "Playing  17  th game\n",
            "Playing  18  th game\n",
            "Playing  19  th game\n",
            "Playing  20  th game\n",
            "Playing  21  th game\n",
            "Playing  22  th game\n",
            "Playing  23  th game\n",
            "Playing  24  th game\n",
            "Playing  25  th game\n",
            "Playing  26  th game\n",
            "Playing  27  th game\n",
            "Playing  28  th game\n",
            "Playing  29  th game\n",
            "Playing  30  th game\n",
            "Playing  31  th game\n",
            "Playing  32  th game\n",
            "Playing  33  th game\n",
            "Playing  34  th game\n",
            "Playing  35  th game\n",
            "Playing  36  th game\n",
            "Playing  37  th game\n",
            "Playing  38  th game\n",
            "Playing  39  th game\n",
            "Playing  40  th game\n",
            "Playing  41  th game\n",
            "Playing  42  th game\n",
            "Playing  43  th game\n",
            "Playing  44  th game\n",
            "Playing  45  th game\n",
            "Playing  46  th game\n",
            "Playing  47  th game\n",
            "Playing  48  th game\n",
            "Playing  49  th game\n",
            "Playing  50  th game\n",
            "Playing  51  th game\n",
            "Playing  52  th game\n",
            "Playing  53  th game\n",
            "Playing  54  th game\n",
            "Playing  55  th game\n",
            "Playing  56  th game\n",
            "Playing  57  th game\n",
            "Playing  58  th game\n",
            "Playing  59  th game\n",
            "Playing  60  th game\n",
            "Playing  61  th game\n",
            "Playing  62  th game\n",
            "Playing  63  th game\n",
            "Playing  64  th game\n",
            "Playing  65  th game\n",
            "Playing  66  th game\n",
            "Playing  67  th game\n",
            "Playing  68  th game\n",
            "Playing  69  th game\n",
            "Playing  70  th game\n",
            "Playing  71  th game\n",
            "Playing  72  th game\n",
            "Playing  73  th game\n",
            "Playing  74  th game\n",
            "Playing  75  th game\n",
            "Playing  76  th game\n",
            "Playing  77  th game\n",
            "Playing  78  th game\n",
            "Playing  79  th game\n",
            "Playing  80  th game\n",
            "Playing  81  th game\n",
            "Playing  82  th game\n",
            "Playing  83  th game\n",
            "Playing  84  th game\n",
            "Playing  85  th game\n",
            "Playing  86  th game\n",
            "Playing  87  th game\n",
            "Playing  88  th game\n",
            "Playing  89  th game\n",
            "Playing  90  th game\n",
            "Playing  91  th game\n",
            "Playing  92  th game\n",
            "Playing  93  th game\n",
            "Playing  94  th game\n",
            "Playing  95  th game\n",
            "Playing  96  th game\n",
            "Playing  97  th game\n",
            "Playing  98  th game\n",
            "Playing  99  th game\n",
            "Playing  100  th game\n",
            "Playing  101  th game\n",
            "Playing  102  th game\n",
            "Playing  103  th game\n",
            "Playing  104  th game\n",
            "Playing  105  th game\n",
            "Playing  106  th game\n",
            "Playing  107  th game\n",
            "Playing  108  th game\n",
            "Playing  109  th game\n",
            "Playing  110  th game\n",
            "Playing  111  th game\n",
            "Playing  112  th game\n",
            "Playing  113  th game\n",
            "Playing  114  th game\n",
            "Playing  115  th game\n",
            "Playing  116  th game\n",
            "Playing  117  th game\n",
            "Playing  118  th game\n",
            "Playing  119  th game\n",
            "Playing  120  th game\n",
            "Playing  121  th game\n",
            "Playing  122  th game\n",
            "Playing  123  th game\n",
            "Playing  124  th game\n",
            "Playing  125  th game\n",
            "Playing  126  th game\n",
            "Playing  127  th game\n",
            "Playing  128  th game\n",
            "Playing  129  th game\n",
            "Playing  130  th game\n",
            "Playing  131  th game\n",
            "Playing  132  th game\n",
            "Playing  133  th game\n",
            "Playing  134  th game\n",
            "Playing  135  th game\n",
            "Playing  136  th game\n",
            "Playing  137  th game\n",
            "Playing  138  th game\n",
            "Playing  139  th game\n",
            "Playing  140  th game\n",
            "Playing  141  th game\n",
            "Playing  142  th game\n",
            "Playing  143  th game\n",
            "Playing  144  th game\n",
            "Playing  145  th game\n",
            "Playing  146  th game\n",
            "Playing  147  th game\n",
            "Playing  148  th game\n",
            "Playing  149  th game\n",
            "Playing  150  th game\n",
            "Playing  151  th game\n",
            "Playing  152  th game\n",
            "Playing  153  th game\n",
            "Playing  154  th game\n",
            "Playing  155  th game\n",
            "Playing  156  th game\n",
            "Playing  157  th game\n",
            "Playing  158  th game\n",
            "Playing  159  th game\n",
            "Playing  160  th game\n",
            "Playing  161  th game\n",
            "Playing  162  th game\n",
            "Playing  163  th game\n",
            "Playing  164  th game\n",
            "Playing  165  th game\n",
            "Playing  166  th game\n",
            "Playing  167  th game\n",
            "Playing  168  th game\n",
            "Playing  169  th game\n",
            "Playing  170  th game\n",
            "Playing  171  th game\n",
            "Playing  172  th game\n",
            "Playing  173  th game\n",
            "Playing  174  th game\n",
            "Playing  175  th game\n",
            "Playing  176  th game\n",
            "Playing  177  th game\n",
            "Playing  178  th game\n",
            "Playing  179  th game\n",
            "Playing  180  th game\n",
            "Playing  181  th game\n",
            "Playing  182  th game\n",
            "Playing  183  th game\n",
            "Playing  184  th game\n",
            "Playing  185  th game\n",
            "Playing  186  th game\n",
            "Playing  187  th game\n",
            "Playing  188  th game\n",
            "Playing  189  th game\n",
            "Playing  190  th game\n",
            "Playing  191  th game\n",
            "Playing  192  th game\n",
            "Playing  193  th game\n",
            "Playing  194  th game\n",
            "Playing  195  th game\n",
            "Playing  196  th game\n",
            "Playing  197  th game\n",
            "Playing  198  th game\n",
            "Playing  199  th game\n",
            "Playing  200  th game\n",
            "Playing  201  th game\n",
            "Playing  202  th game\n",
            "Playing  203  th game\n",
            "Playing  204  th game\n",
            "Playing  205  th game\n",
            "Playing  206  th game\n",
            "Playing  207  th game\n",
            "Playing  208  th game\n",
            "Playing  209  th game\n",
            "Playing  210  th game\n",
            "Playing  211  th game\n",
            "Playing  212  th game\n",
            "Playing  213  th game\n",
            "Playing  214  th game\n",
            "Playing  215  th game\n",
            "Playing  216  th game\n",
            "Playing  217  th game\n",
            "Playing  218  th game\n",
            "Playing  219  th game\n",
            "Playing  220  th game\n",
            "Playing  221  th game\n",
            "Playing  222  th game\n",
            "Playing  223  th game\n",
            "Playing  224  th game\n",
            "Playing  225  th game\n",
            "Playing  226  th game\n",
            "Playing  227  th game\n",
            "Playing  228  th game\n",
            "Playing  229  th game\n",
            "Playing  230  th game\n",
            "Playing  231  th game\n",
            "Playing  232  th game\n",
            "Playing  233  th game\n",
            "Playing  234  th game\n",
            "Playing  235  th game\n",
            "Playing  236  th game\n",
            "Playing  237  th game\n",
            "Playing  238  th game\n",
            "Playing  239  th game\n",
            "Playing  240  th game\n",
            "Playing  241  th game\n",
            "Playing  242  th game\n",
            "Playing  243  th game\n",
            "Playing  244  th game\n",
            "Playing  245  th game\n",
            "Playing  246  th game\n",
            "Playing  247  th game\n",
            "Playing  248  th game\n",
            "Playing  249  th game\n",
            "Playing  250  th game\n",
            "Playing  251  th game\n",
            "Playing  252  th game\n",
            "Playing  253  th game\n",
            "Playing  254  th game\n",
            "Playing  255  th game\n",
            "Playing  256  th game\n",
            "Playing  257  th game\n",
            "Playing  258  th game\n",
            "Playing  259  th game\n",
            "Playing  260  th game\n",
            "Playing  261  th game\n",
            "Playing  262  th game\n",
            "Playing  263  th game\n",
            "Playing  264  th game\n",
            "Playing  265  th game\n",
            "Playing  266  th game\n",
            "Playing  267  th game\n",
            "Playing  268  th game\n",
            "Playing  269  th game\n",
            "Playing  270  th game\n",
            "Playing  271  th game\n",
            "Playing  272  th game\n",
            "Playing  273  th game\n",
            "Playing  274  th game\n",
            "Playing  275  th game\n",
            "Playing  276  th game\n",
            "Playing  277  th game\n",
            "Playing  278  th game\n",
            "Playing  279  th game\n",
            "Playing  280  th game\n",
            "Playing  281  th game\n",
            "Playing  282  th game\n",
            "Playing  283  th game\n",
            "Playing  284  th game\n",
            "Playing  285  th game\n",
            "Playing  286  th game\n",
            "Playing  287  th game\n",
            "Playing  288  th game\n",
            "Playing  289  th game\n",
            "Playing  290  th game\n",
            "Playing  291  th game\n",
            "Playing  292  th game\n",
            "Playing  293  th game\n",
            "Playing  294  th game\n",
            "Playing  295  th game\n",
            "Playing  296  th game\n",
            "Playing  297  th game\n",
            "Playing  298  th game\n",
            "Playing  299  th game\n",
            "Playing  300  th game\n",
            "Playing  301  th game\n",
            "Playing  302  th game\n",
            "Playing  303  th game\n",
            "Playing  304  th game\n",
            "Playing  305  th game\n",
            "Playing  306  th game\n",
            "Playing  307  th game\n",
            "Playing  308  th game\n",
            "Playing  309  th game\n",
            "Playing  310  th game\n",
            "Playing  311  th game\n",
            "Playing  312  th game\n",
            "Playing  313  th game\n",
            "Playing  314  th game\n",
            "Playing  315  th game\n",
            "Playing  316  th game\n",
            "Playing  317  th game\n",
            "Playing  318  th game\n",
            "Playing  319  th game\n",
            "Playing  320  th game\n",
            "Playing  321  th game\n",
            "Playing  322  th game\n",
            "Playing  323  th game\n",
            "Playing  324  th game\n",
            "Playing  325  th game\n",
            "Playing  326  th game\n",
            "Playing  327  th game\n",
            "Playing  328  th game\n",
            "Playing  329  th game\n",
            "Playing  330  th game\n",
            "Playing  331  th game\n",
            "Playing  332  th game\n",
            "Playing  333  th game\n",
            "Playing  334  th game\n",
            "Playing  335  th game\n",
            "Playing  336  th game\n",
            "Playing  337  th game\n",
            "Playing  338  th game\n",
            "Playing  339  th game\n",
            "Playing  340  th game\n",
            "Playing  341  th game\n",
            "Playing  342  th game\n",
            "Playing  343  th game\n",
            "Playing  344  th game\n",
            "Playing  345  th game\n",
            "Playing  346  th game\n",
            "Playing  347  th game\n",
            "Playing  348  th game\n",
            "Playing  349  th game\n",
            "Playing  350  th game\n",
            "Playing  351  th game\n",
            "Playing  352  th game\n",
            "Playing  353  th game\n",
            "Playing  354  th game\n",
            "Playing  355  th game\n",
            "Playing  356  th game\n",
            "Playing  357  th game\n",
            "Playing  358  th game\n",
            "Playing  359  th game\n",
            "Playing  360  th game\n",
            "Playing  361  th game\n",
            "Playing  362  th game\n",
            "Playing  363  th game\n",
            "Playing  364  th game\n",
            "Playing  365  th game\n",
            "Playing  366  th game\n",
            "Playing  367  th game\n",
            "Playing  368  th game\n",
            "Playing  369  th game\n",
            "Playing  370  th game\n",
            "Playing  371  th game\n",
            "Playing  372  th game\n",
            "Playing  373  th game\n",
            "Playing  374  th game\n",
            "Playing  375  th game\n",
            "Playing  376  th game\n",
            "Playing  377  th game\n",
            "Playing  378  th game\n",
            "Playing  379  th game\n",
            "Playing  380  th game\n",
            "Playing  381  th game\n",
            "Playing  382  th game\n",
            "Playing  383  th game\n",
            "Playing  384  th game\n",
            "Playing  385  th game\n",
            "Playing  386  th game\n",
            "Playing  387  th game\n",
            "Playing  388  th game\n",
            "Playing  389  th game\n",
            "Playing  390  th game\n",
            "Playing  391  th game\n",
            "Playing  392  th game\n",
            "Playing  393  th game\n",
            "Playing  394  th game\n",
            "Playing  395  th game\n",
            "Playing  396  th game\n",
            "Playing  397  th game\n",
            "Playing  398  th game\n",
            "Playing  399  th game\n",
            "Playing  400  th game\n",
            "Playing  401  th game\n",
            "Playing  402  th game\n",
            "Playing  403  th game\n",
            "Playing  404  th game\n",
            "Playing  405  th game\n",
            "Playing  406  th game\n",
            "Playing  407  th game\n",
            "Playing  408  th game\n",
            "Playing  409  th game\n",
            "Playing  410  th game\n",
            "Playing  411  th game\n",
            "Playing  412  th game\n",
            "Playing  413  th game\n",
            "Playing  414  th game\n",
            "Playing  415  th game\n",
            "Playing  416  th game\n",
            "Playing  417  th game\n",
            "Playing  418  th game\n",
            "Playing  419  th game\n",
            "Playing  420  th game\n",
            "Playing  421  th game\n",
            "Playing  422  th game\n",
            "Playing  423  th game\n",
            "Playing  424  th game\n",
            "Playing  425  th game\n",
            "Playing  426  th game\n",
            "Playing  427  th game\n",
            "Playing  428  th game\n",
            "Playing  429  th game\n",
            "Playing  430  th game\n",
            "Playing  431  th game\n",
            "Playing  432  th game\n",
            "Playing  433  th game\n",
            "Playing  434  th game\n",
            "Playing  435  th game\n",
            "Playing  436  th game\n",
            "Playing  437  th game\n",
            "Playing  438  th game\n",
            "Playing  439  th game\n",
            "Playing  440  th game\n",
            "Playing  441  th game\n",
            "Playing  442  th game\n",
            "Playing  443  th game\n",
            "Playing  444  th game\n",
            "Playing  445  th game\n",
            "Playing  446  th game\n",
            "Playing  447  th game\n",
            "Playing  448  th game\n",
            "Playing  449  th game\n",
            "Playing  450  th game\n",
            "Playing  451  th game\n",
            "Playing  452  th game\n",
            "Playing  453  th game\n",
            "Playing  454  th game\n",
            "Playing  455  th game\n",
            "Playing  456  th game\n",
            "Playing  457  th game\n",
            "Playing  458  th game\n",
            "Playing  459  th game\n",
            "Playing  460  th game\n",
            "Playing  461  th game\n",
            "Playing  462  th game\n",
            "Playing  463  th game\n",
            "Playing  464  th game\n",
            "Playing  465  th game\n",
            "Playing  466  th game\n",
            "Playing  467  th game\n",
            "Playing  468  th game\n",
            "Playing  469  th game\n",
            "Playing  470  th game\n",
            "Playing  471  th game\n",
            "Playing  472  th game\n",
            "Playing  473  th game\n",
            "Playing  474  th game\n",
            "Playing  475  th game\n",
            "Playing  476  th game\n",
            "Playing  477  th game\n",
            "Playing  478  th game\n",
            "Playing  479  th game\n",
            "Playing  480  th game\n",
            "Playing  481  th game\n",
            "Playing  482  th game\n",
            "Playing  483  th game\n",
            "Playing  484  th game\n",
            "Playing  485  th game\n",
            "Playing  486  th game\n",
            "Playing  487  th game\n",
            "Playing  488  th game\n",
            "Playing  489  th game\n",
            "Playing  490  th game\n",
            "Playing  491  th game\n",
            "Playing  492  th game\n",
            "Playing  493  th game\n",
            "Playing  494  th game\n",
            "Playing  495  th game\n",
            "Playing  496  th game\n",
            "Playing  497  th game\n",
            "Playing  498  th game\n",
            "Playing  499  th game\n",
            "Playing  500  th game\n",
            "Playing  501  th game\n",
            "Playing  502  th game\n",
            "Playing  503  th game\n",
            "Playing  504  th game\n",
            "Playing  505  th game\n",
            "Playing  506  th game\n",
            "Playing  507  th game\n",
            "Playing  508  th game\n",
            "Playing  509  th game\n",
            "Playing  510  th game\n",
            "Playing  511  th game\n",
            "Playing  512  th game\n",
            "Playing  513  th game\n",
            "Playing  514  th game\n",
            "Playing  515  th game\n",
            "Playing  516  th game\n",
            "Playing  517  th game\n",
            "Playing  518  th game\n",
            "Playing  519  th game\n",
            "Playing  520  th game\n",
            "Playing  521  th game\n",
            "Playing  522  th game\n",
            "Playing  523  th game\n",
            "Playing  524  th game\n",
            "Playing  525  th game\n",
            "Playing  526  th game\n",
            "Playing  527  th game\n",
            "Playing  528  th game\n",
            "Playing  529  th game\n",
            "Playing  530  th game\n",
            "Playing  531  th game\n",
            "Playing  532  th game\n",
            "Playing  533  th game\n",
            "Playing  534  th game\n",
            "Playing  535  th game\n",
            "Playing  536  th game\n",
            "Playing  537  th game\n",
            "Playing  538  th game\n",
            "Playing  539  th game\n",
            "Playing  540  th game\n",
            "Playing  541  th game\n",
            "Playing  542  th game\n",
            "Playing  543  th game\n",
            "Playing  544  th game\n",
            "Playing  545  th game\n",
            "Playing  546  th game\n",
            "Playing  547  th game\n",
            "Playing  548  th game\n",
            "Playing  549  th game\n",
            "Playing  550  th game\n",
            "Playing  551  th game\n",
            "Playing  552  th game\n",
            "Playing  553  th game\n",
            "Playing  554  th game\n",
            "Playing  555  th game\n",
            "Playing  556  th game\n",
            "Playing  557  th game\n",
            "Playing  558  th game\n",
            "Playing  559  th game\n",
            "Playing  560  th game\n",
            "Playing  561  th game\n",
            "Playing  562  th game\n",
            "Playing  563  th game\n",
            "Playing  564  th game\n",
            "Playing  565  th game\n",
            "Playing  566  th game\n",
            "Playing  567  th game\n",
            "Playing  568  th game\n",
            "Playing  569  th game\n",
            "Playing  570  th game\n",
            "Playing  571  th game\n",
            "Playing  572  th game\n",
            "Playing  573  th game\n",
            "Playing  574  th game\n",
            "Playing  575  th game\n",
            "Playing  576  th game\n",
            "Playing  577  th game\n",
            "Playing  578  th game\n",
            "Playing  579  th game\n",
            "Playing  580  th game\n",
            "Playing  581  th game\n",
            "Playing  582  th game\n",
            "Playing  583  th game\n",
            "Playing  584  th game\n",
            "Playing  585  th game\n",
            "Playing  586  th game\n",
            "Playing  587  th game\n",
            "Playing  588  th game\n",
            "Playing  589  th game\n",
            "Playing  590  th game\n",
            "Playing  591  th game\n",
            "Playing  592  th game\n",
            "Playing  593  th game\n",
            "Playing  594  th game\n",
            "Playing  595  th game\n",
            "Playing  596  th game\n",
            "Playing  597  th game\n",
            "Playing  598  th game\n",
            "Playing  599  th game\n",
            "Playing  600  th game\n",
            "Playing  601  th game\n",
            "Playing  602  th game\n",
            "Playing  603  th game\n",
            "Playing  604  th game\n",
            "Playing  605  th game\n",
            "Playing  606  th game\n",
            "Playing  607  th game\n",
            "Playing  608  th game\n",
            "Playing  609  th game\n",
            "Playing  610  th game\n",
            "Playing  611  th game\n",
            "Playing  612  th game\n",
            "Playing  613  th game\n",
            "Playing  614  th game\n",
            "Playing  615  th game\n",
            "Playing  616  th game\n",
            "Playing  617  th game\n",
            "Playing  618  th game\n",
            "Playing  619  th game\n",
            "Playing  620  th game\n",
            "Playing  621  th game\n",
            "Playing  622  th game\n",
            "Playing  623  th game\n",
            "Playing  624  th game\n",
            "Playing  625  th game\n",
            "Playing  626  th game\n",
            "Playing  627  th game\n",
            "Playing  628  th game\n",
            "Playing  629  th game\n",
            "Playing  630  th game\n",
            "Playing  631  th game\n",
            "Playing  632  th game\n",
            "Playing  633  th game\n",
            "Playing  634  th game\n",
            "Playing  635  th game\n",
            "Playing  636  th game\n",
            "Playing  637  th game\n",
            "Playing  638  th game\n",
            "Playing  639  th game\n",
            "Playing  640  th game\n",
            "Playing  641  th game\n",
            "Playing  642  th game\n",
            "Playing  643  th game\n",
            "Playing  644  th game\n",
            "Playing  645  th game\n",
            "Playing  646  th game\n",
            "Playing  647  th game\n",
            "Playing  648  th game\n",
            "Playing  649  th game\n",
            "Playing  650  th game\n",
            "Playing  651  th game\n",
            "Playing  652  th game\n",
            "Playing  653  th game\n",
            "Playing  654  th game\n",
            "Playing  655  th game\n",
            "Playing  656  th game\n",
            "Playing  657  th game\n",
            "Playing  658  th game\n",
            "Playing  659  th game\n",
            "Playing  660  th game\n",
            "Playing  661  th game\n",
            "Playing  662  th game\n",
            "Playing  663  th game\n",
            "Playing  664  th game\n",
            "Playing  665  th game\n",
            "Playing  666  th game\n",
            "Playing  667  th game\n",
            "Playing  668  th game\n",
            "Playing  669  th game\n",
            "Playing  670  th game\n",
            "Playing  671  th game\n",
            "Playing  672  th game\n",
            "Playing  673  th game\n",
            "Playing  674  th game\n",
            "Playing  675  th game\n",
            "Playing  676  th game\n",
            "Playing  677  th game\n",
            "Playing  678  th game\n",
            "Playing  679  th game\n",
            "Playing  680  th game\n",
            "Playing  681  th game\n",
            "Playing  682  th game\n",
            "Playing  683  th game\n",
            "Playing  684  th game\n",
            "Playing  685  th game\n",
            "Playing  686  th game\n",
            "Playing  687  th game\n",
            "Playing  688  th game\n",
            "Playing  689  th game\n",
            "Playing  690  th game\n",
            "Playing  691  th game\n",
            "Playing  692  th game\n",
            "Playing  693  th game\n",
            "Playing  694  th game\n",
            "Playing  695  th game\n",
            "Playing  696  th game\n",
            "Playing  697  th game\n",
            "Playing  698  th game\n",
            "Playing  699  th game\n",
            "Playing  700  th game\n",
            "Playing  701  th game\n",
            "Playing  702  th game\n",
            "Playing  703  th game\n",
            "Playing  704  th game\n",
            "Playing  705  th game\n",
            "Playing  706  th game\n",
            "Playing  707  th game\n",
            "Playing  708  th game\n",
            "Playing  709  th game\n",
            "Playing  710  th game\n",
            "Playing  711  th game\n",
            "Playing  712  th game\n",
            "Playing  713  th game\n",
            "Playing  714  th game\n",
            "Playing  715  th game\n",
            "Playing  716  th game\n",
            "Playing  717  th game\n",
            "Playing  718  th game\n",
            "Playing  719  th game\n",
            "Playing  720  th game\n",
            "Playing  721  th game\n",
            "Playing  722  th game\n",
            "Playing  723  th game\n",
            "Playing  724  th game\n",
            "Playing  725  th game\n",
            "Playing  726  th game\n",
            "Playing  727  th game\n",
            "Playing  728  th game\n",
            "Playing  729  th game\n",
            "Playing  730  th game\n",
            "Playing  731  th game\n",
            "Playing  732  th game\n",
            "Playing  733  th game\n",
            "Playing  734  th game\n",
            "Playing  735  th game\n",
            "Playing  736  th game\n",
            "Playing  737  th game\n",
            "Playing  738  th game\n",
            "Playing  739  th game\n",
            "Playing  740  th game\n",
            "Playing  741  th game\n",
            "Playing  742  th game\n",
            "Playing  743  th game\n",
            "Playing  744  th game\n",
            "Playing  745  th game\n",
            "Playing  746  th game\n",
            "Playing  747  th game\n",
            "Playing  748  th game\n",
            "Playing  749  th game\n",
            "Playing  750  th game\n",
            "Playing  751  th game\n",
            "Playing  752  th game\n",
            "Playing  753  th game\n",
            "Playing  754  th game\n",
            "Playing  755  th game\n",
            "Playing  756  th game\n",
            "Playing  757  th game\n",
            "Playing  758  th game\n",
            "Playing  759  th game\n",
            "Playing  760  th game\n",
            "Playing  761  th game\n",
            "Playing  762  th game\n",
            "Playing  763  th game\n",
            "Playing  764  th game\n",
            "Playing  765  th game\n",
            "Playing  766  th game\n",
            "Playing  767  th game\n",
            "Playing  768  th game\n",
            "Playing  769  th game\n",
            "Playing  770  th game\n",
            "Playing  771  th game\n",
            "Playing  772  th game\n",
            "Playing  773  th game\n",
            "Playing  774  th game\n",
            "Playing  775  th game\n",
            "Playing  776  th game\n",
            "Playing  777  th game\n",
            "Playing  778  th game\n",
            "Playing  779  th game\n",
            "Playing  780  th game\n",
            "Playing  781  th game\n",
            "Playing  782  th game\n",
            "Playing  783  th game\n",
            "Playing  784  th game\n",
            "Playing  785  th game\n",
            "Playing  786  th game\n",
            "Playing  787  th game\n",
            "Playing  788  th game\n",
            "Playing  789  th game\n",
            "Playing  790  th game\n",
            "Playing  791  th game\n",
            "Playing  792  th game\n",
            "Playing  793  th game\n",
            "Playing  794  th game\n",
            "Playing  795  th game\n",
            "Playing  796  th game\n",
            "Playing  797  th game\n",
            "Playing  798  th game\n",
            "Playing  799  th game\n",
            "Playing  800  th game\n",
            "Playing  801  th game\n",
            "Playing  802  th game\n",
            "Playing  803  th game\n",
            "Playing  804  th game\n",
            "Playing  805  th game\n",
            "Playing  806  th game\n",
            "Playing  807  th game\n",
            "Playing  808  th game\n",
            "Playing  809  th game\n",
            "Playing  810  th game\n",
            "Playing  811  th game\n",
            "Playing  812  th game\n",
            "Playing  813  th game\n",
            "Playing  814  th game\n",
            "Playing  815  th game\n",
            "Playing  816  th game\n",
            "Playing  817  th game\n",
            "Playing  818  th game\n",
            "Playing  819  th game\n",
            "Playing  820  th game\n",
            "Playing  821  th game\n",
            "Playing  822  th game\n",
            "Playing  823  th game\n",
            "Playing  824  th game\n",
            "Playing  825  th game\n",
            "Playing  826  th game\n",
            "Playing  827  th game\n",
            "Playing  828  th game\n",
            "Playing  829  th game\n",
            "Playing  830  th game\n",
            "Playing  831  th game\n",
            "Playing  832  th game\n",
            "Playing  833  th game\n",
            "Playing  834  th game\n",
            "Playing  835  th game\n",
            "Playing  836  th game\n",
            "Playing  837  th game\n",
            "Playing  838  th game\n",
            "Playing  839  th game\n",
            "Playing  840  th game\n",
            "Playing  841  th game\n",
            "Playing  842  th game\n",
            "Playing  843  th game\n",
            "Playing  844  th game\n",
            "Playing  845  th game\n",
            "Playing  846  th game\n",
            "Playing  847  th game\n",
            "Playing  848  th game\n",
            "Playing  849  th game\n",
            "Playing  850  th game\n",
            "Playing  851  th game\n",
            "Playing  852  th game\n",
            "Playing  853  th game\n",
            "Playing  854  th game\n",
            "Playing  855  th game\n",
            "Playing  856  th game\n",
            "Playing  857  th game\n",
            "Playing  858  th game\n",
            "Playing  859  th game\n",
            "Playing  860  th game\n",
            "Playing  861  th game\n",
            "Playing  862  th game\n",
            "Playing  863  th game\n",
            "Playing  864  th game\n",
            "Playing  865  th game\n",
            "Playing  866  th game\n",
            "Playing  867  th game\n",
            "Playing  868  th game\n",
            "Playing  869  th game\n",
            "Playing  870  th game\n",
            "Playing  871  th game\n",
            "Playing  872  th game\n",
            "Playing  873  th game\n",
            "Playing  874  th game\n",
            "Playing  875  th game\n",
            "Playing  876  th game\n",
            "Playing  877  th game\n",
            "Playing  878  th game\n",
            "Playing  879  th game\n",
            "Playing  880  th game\n",
            "Playing  881  th game\n",
            "Playing  882  th game\n",
            "Playing  883  th game\n",
            "Playing  884  th game\n",
            "Playing  885  th game\n",
            "Playing  886  th game\n",
            "Playing  887  th game\n",
            "Playing  888  th game\n",
            "Playing  889  th game\n",
            "Playing  890  th game\n",
            "Playing  891  th game\n",
            "Playing  892  th game\n",
            "Playing  893  th game\n",
            "Playing  894  th game\n",
            "Playing  895  th game\n",
            "Playing  896  th game\n",
            "Playing  897  th game\n",
            "Playing  898  th game\n",
            "Playing  899  th game\n",
            "Playing  900  th game\n",
            "Playing  901  th game\n",
            "Playing  902  th game\n",
            "Playing  903  th game\n",
            "Playing  904  th game\n",
            "Playing  905  th game\n",
            "Playing  906  th game\n",
            "Playing  907  th game\n",
            "Playing  908  th game\n",
            "Playing  909  th game\n",
            "Playing  910  th game\n",
            "Playing  911  th game\n",
            "Playing  912  th game\n",
            "Playing  913  th game\n",
            "Playing  914  th game\n",
            "Playing  915  th game\n",
            "Playing  916  th game\n",
            "Playing  917  th game\n",
            "Playing  918  th game\n",
            "Playing  919  th game\n",
            "Playing  920  th game\n",
            "Playing  921  th game\n",
            "Playing  922  th game\n",
            "Playing  923  th game\n",
            "Playing  924  th game\n",
            "Playing  925  th game\n",
            "Playing  926  th game\n",
            "Playing  927  th game\n",
            "Playing  928  th game\n",
            "Playing  929  th game\n",
            "Playing  930  th game\n",
            "Playing  931  th game\n",
            "Playing  932  th game\n",
            "Playing  933  th game\n",
            "Playing  934  th game\n",
            "Playing  935  th game\n",
            "Playing  936  th game\n",
            "Playing  937  th game\n",
            "Playing  938  th game\n",
            "Playing  939  th game\n",
            "Playing  940  th game\n",
            "Playing  941  th game\n",
            "Playing  942  th game\n",
            "Playing  943  th game\n",
            "Playing  944  th game\n",
            "Playing  945  th game\n",
            "Playing  946  th game\n",
            "Playing  947  th game\n",
            "Playing  948  th game\n",
            "Playing  949  th game\n",
            "Playing  950  th game\n",
            "Playing  951  th game\n",
            "Playing  952  th game\n",
            "Playing  953  th game\n",
            "Playing  954  th game\n",
            "Playing  955  th game\n",
            "Playing  956  th game\n",
            "Playing  957  th game\n",
            "Playing  958  th game\n",
            "Playing  959  th game\n",
            "Playing  960  th game\n",
            "Playing  961  th game\n",
            "Playing  962  th game\n",
            "Playing  963  th game\n",
            "Playing  964  th game\n",
            "Playing  965  th game\n",
            "Playing  966  th game\n",
            "Playing  967  th game\n",
            "Playing  968  th game\n",
            "Playing  969  th game\n",
            "Playing  970  th game\n",
            "Playing  971  th game\n",
            "Playing  972  th game\n",
            "Playing  973  th game\n",
            "Playing  974  th game\n",
            "Playing  975  th game\n",
            "Playing  976  th game\n",
            "Playing  977  th game\n",
            "Playing  978  th game\n",
            "Playing  979  th game\n",
            "Playing  980  th game\n",
            "Playing  981  th game\n",
            "Playing  982  th game\n",
            "Playing  983  th game\n",
            "Playing  984  th game\n",
            "Playing  985  th game\n",
            "Playing  986  th game\n",
            "Playing  987  th game\n",
            "Playing  988  th game\n",
            "Playing  989  th game\n",
            "Playing  990  th game\n",
            "Playing  991  th game\n",
            "Playing  992  th game\n",
            "Playing  993  th game\n",
            "Playing  994  th game\n",
            "Playing  995  th game\n",
            "Playing  996  th game\n",
            "Playing  997  th game\n",
            "Playing  998  th game\n",
            "Playing  999  th game\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HangmanAPIError",
          "evalue": "{'error': 'You have reached 1000 of games', 'status': 'denied'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHangmanAPIError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-25daa8a5afde>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Playing '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' th game'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpractice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-e7b911fce3af>\u001b[0m in \u001b[0;36mstart_game\u001b[0;34m(self, practice, verbose)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/new_game\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"practice\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpractice\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"approved\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mgame_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'game_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-e7b911fce3af>\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, path, args, post_args, method)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHangmanAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHangmanAPIError\u001b[0m: {'error': 'You have reached 1000 of games', 'status': 'denied'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qtzEcX8_ppuk"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}